{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import math\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_json_set_SINGLE(df):\n",
    "    microtask_jsons = []\n",
    "    padding = 100\n",
    "    try:\n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            for idy, evidence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "                \n",
    "                # Let us retrieve the sections before and after the evidence. 100 characters to each side will do.\n",
    "                all_sentences = row['nlp_sentences']    \n",
    "                \n",
    "                evidence_idx = evidence['sentence_id']\n",
    "                if ';' in evidence_idx:\n",
    "                    before_idx = int(evidence_idx.split(';')[0])-1\n",
    "                    after_idx = int(evidence_idx.split(';')[1])+1\n",
    "                else:                    \n",
    "                    before_idx = int(evidence_idx)-1\n",
    "                    after_idx = int(evidence_idx)+1\n",
    "                    \n",
    "                pre_evidence = []\n",
    "                for i in range(before_idx,-1,-1):\n",
    "                    pre_evidence.insert(0, all_sentences[i])\n",
    "                    if len(' '.join(pre_evidence))>=100:\n",
    "                        break\n",
    "                pre_evidence = ' '.join(pre_evidence)\n",
    "                \n",
    "                post_evidence = []\n",
    "                for i in range(after_idx,len(all_sentences)):\n",
    "                    post_evidence.append(all_sentences[i])\n",
    "                    if len(' '.join(post_evidence))>=100:\n",
    "                        break\n",
    "                post_evidence = ' '.join(post_evidence)\n",
    "                \n",
    "                assert ' '.join([pre_evidence, evidence['sentence'], post_evidence]).strip() in ' '.join(all_sentences)\n",
    "                \n",
    "                microtask_json = {\n",
    "                    \"claim_id\": row['claim_id'],\n",
    "                    \"reference_id\": row['reference_id'],\n",
    "                    \"evidence_id\": idy,\n",
    "                    \"sentence_id\": evidence_idx,\n",
    "                    \"score\": evidence['score'],\n",
    "                    \"affirmation\": row['final_verbalisation'],\n",
    "                    \"pre_evidence\": pre_evidence,\n",
    "                    \"evidence\": evidence['sentence'],\n",
    "                    \"post_evidence\": post_evidence,\n",
    "                    \"reference_url\": row['final_url'],\n",
    "                    \"g_id\": -1\n",
    "                }\n",
    "                microtask_jsons.append(microtask_json)\n",
    "                #raise ValueError\n",
    "                \n",
    "        return microtask_jsons\n",
    "    except Exception:\n",
    "        print(' '.join([pre_evidence, evidence['sentence'], post_evidence]))\n",
    "        #print(pre_evidence)\n",
    "        #print('--')\n",
    "        #print(evidence['sentence'])\n",
    "        #print('--')\n",
    "        #print(post_evidence)\n",
    "        print('--')\n",
    "        print(' '.join(all_sentences))\n",
    "        #print(row)\n",
    "        #traceback.print_exc()\n",
    "        #pdb.set_trace()\n",
    "        raise\n",
    "        \n",
    "def convert_df_to_json_set_MULTIPLE(df):\n",
    "    microtask_jsons = []\n",
    "    padding = 100\n",
    "    try:\n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            evidence_list = []\n",
    "            all_sentences = row['nlp_sentences']\n",
    "            \n",
    "            for idy, evidence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "                              \n",
    "                evidence_idx = evidence['sentence_id']\n",
    "                if ';' in evidence_idx:\n",
    "                    before_idx = int(evidence_idx.split(';')[0])-1\n",
    "                    after_idx = int(evidence_idx.split(';')[1])+1\n",
    "                else:                    \n",
    "                    before_idx = int(evidence_idx)-1\n",
    "                    after_idx = int(evidence_idx)+1\n",
    "                    \n",
    "                pre_evidence = []\n",
    "                for i in range(before_idx,-1,-1):\n",
    "                    pre_evidence.insert(0, all_sentences[i])\n",
    "                    if len(' '.join(pre_evidence))>=100:\n",
    "                        break\n",
    "                pre_evidence = ' '.join(pre_evidence)\n",
    "                \n",
    "                post_evidence = []\n",
    "                for i in range(after_idx,len(all_sentences)):\n",
    "                    post_evidence.append(all_sentences[i])\n",
    "                    if len(' '.join(post_evidence))>=100:\n",
    "                        break\n",
    "                post_evidence = ' '.join(post_evidence)\n",
    "                \n",
    "                assert ' '.join([pre_evidence, evidence['sentence'], post_evidence]).strip() in ' '.join(all_sentences)\n",
    "                \n",
    "                evidence_list.append({\n",
    "                    \"evidence_id\": idy,\n",
    "                    \"sentence_id\": evidence_idx,\n",
    "                    \"score\": evidence['score'],\n",
    "                    \"pre_evidence\": pre_evidence,\n",
    "                    \"evidence\": evidence['sentence'],\n",
    "                    \"post_evidence\": post_evidence,\n",
    "                })\n",
    "            evidence_list_shuffle = random.Random(42).sample(evidence_list, len(evidence_list))    \n",
    "            microtask_json = {\n",
    "                \"claim_id\": row['claim_id'],\n",
    "                \"reference_id\": row['reference_id'],\n",
    "                \"affirmation\": row['final_verbalisation'],\n",
    "                \"reference_url\": row['final_url'],\n",
    "                \"g_id\": -1,\n",
    "                \"evidence_list\": evidence_list_shuffle\n",
    "            }\n",
    "            microtask_jsons.append(microtask_json)\n",
    "                \n",
    "        return microtask_jsons\n",
    "    except Exception:\n",
    "        print(' '.join([pre_evidence, evidence['sentence'], post_evidence]))\n",
    "        #print(pre_evidence)\n",
    "        #print('--')\n",
    "        #print(evidence['sentence'])\n",
    "        #print('--')\n",
    "        #print(post_evidence)\n",
    "        print('--')\n",
    "        print(' '.join(all_sentences))\n",
    "        #print(row)\n",
    "        #traceback.print_exc()\n",
    "        #pdb.set_trace()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "#maxSingleInstances_gd = 2\n",
    "\n",
    "def getRandomTask(counter, n, non_gd_references, maxSingleInstances):\n",
    "    p = [max(maxSingleInstances-c,0.0001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(non_gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )        \n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = non_gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def getRandomGoldenTask(counter, n, gd_references, maxSingleInstances_gd):    \n",
    "    p = [max(maxSingleInstances_gd-c,0.0001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )\n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def generateTaskSet(counter, counter_gd, non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd, n=(4,2)):\n",
    "    '''\n",
    "    counter = a counter which keeps track of how many times each reference was retrieved\n",
    "    n = (x,y) where x = number of non_gd references and y = number of gd references\n",
    "    '''\n",
    "    taskSet = []\n",
    "    task = getRandomTask(counter, n[0], non_gd_references, maxSingleInstances)\n",
    "    taskSet += [r for (r,i) in task] # pairs reference,index are generated here, so that we can update the counter later\n",
    "    task_gd = getRandomGoldenTask(counter_gd, n[1], gd_references, maxSingleInstances_gd)\n",
    "    taskSet += [r for (r,i) in task_gd]\n",
    "    taskSet_shuffle = random.Random(42).sample(taskSet, len(taskSet))\n",
    "    return taskSet_shuffle, [i for (p,i) in task], [i for (p,i) in task_gd] # we return indices here to update counter\n",
    "\n",
    "\n",
    "def generateIDdTaskSets(non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd):\n",
    "    counter = [0]*len(non_gd_references)\n",
    "    counter_gd = [0]*len(gd_references)\n",
    "    taskSets = []\n",
    "    while (any([c < maxSingleInstances for c in counter])):\n",
    "        taskSet, indexes, indexes_gd = generateTaskSet(counter, counter_gd, non_gd_references,\n",
    "                                                       gd_references, maxSingleInstances, maxSingleInstances_gd)\n",
    "        taskSetIDd = {\n",
    "            '_id': str(uuid.uuid4()),\n",
    "            'taskSet' : taskSet\n",
    "        }\n",
    "        taskSets.append(taskSetIDd)\n",
    "        for i in indexes:\n",
    "            counter[i] = counter[i] + 1\n",
    "        for i in indexes_gd:\n",
    "            counter_gd[i] = counter_gd[i] + 1\n",
    "    return taskSets, counter, counter_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1302 entries, 0 to 1301\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           1302 non-null   object \n",
      " 1   claim_id                               1302 non-null   object \n",
      " 2   final_verbalisation                    1302 non-null   object \n",
      " 3   sampling_weight                        1302 non-null   float64\n",
      " 4   final_url                              1302 non-null   object \n",
      " 5   netloc_agg                             1302 non-null   object \n",
      " 6   nlp_sentences                          1302 non-null   object \n",
      " 7   nlp_sentences_slide_2                  1302 non-null   object \n",
      " 8   nlp_sentences_scores                   1302 non-null   object \n",
      " 9   nlp_sentences_slide_2_scores           1302 non-null   object \n",
      " 10  nlp_sentences_all_TOP_N                1302 non-null   object \n",
      " 11  evidence_TE_prob_all_TOP_N             1302 non-null   object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    1302 non-null   object \n",
      " 13  evidence_TE_labels_all_TOP_N           1302 non-null   object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   1302 non-null   object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  1302 non-null   object \n",
      " 16  claim_TE_label_malon_all_TOP_N         1302 non-null   object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 173.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_json('../data/textual_entailment_df.json')\n",
    "data_df = data_df.drop([c for c in data_df.columns if 'TOP_N' in c and 'all_TOP_N' not in c], axis=1)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated cost of running **single evidence** tasks for all 1302 verbalisations, paying 0.5 per task is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4848.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((1302*5)-45)/4*5*0.5*1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a lot, so we will cut this down by:\n",
    "- Restricting only a single random claim per reference_id, returning the balance to referenced websites\n",
    "- Removing the lowest scoring websites according to netloc_agg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 648 entries, 0 to 647\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           648 non-null    object \n",
      " 1   claim_id                               648 non-null    object \n",
      " 2   final_verbalisation                    648 non-null    object \n",
      " 3   sampling_weight                        648 non-null    float64\n",
      " 4   final_url                              648 non-null    object \n",
      " 5   netloc_agg                             648 non-null    object \n",
      " 6   nlp_sentences                          648 non-null    object \n",
      " 7   nlp_sentences_slide_2                  648 non-null    object \n",
      " 8   nlp_sentences_scores                   648 non-null    object \n",
      " 9   nlp_sentences_slide_2_scores           648 non-null    object \n",
      " 10  nlp_sentences_all_TOP_N                648 non-null    object \n",
      " 11  evidence_TE_prob_all_TOP_N             648 non-null    object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    648 non-null    object \n",
      " 13  evidence_TE_labels_all_TOP_N           648 non-null    object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   648 non-null    object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  648 non-null    object \n",
      " 16  claim_TE_label_malon_all_TOP_N         648 non-null    object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df_trim = data_df.sample(frac=1, random_state=42).drop_duplicates('reference_id').sort_index().reset_index(drop=True)\n",
    "data_df_trim.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to previous analysis, these are the worst-performing netloc_agg values in terms of evidence scoring:\n",
    "- witches.shca.ed.ac.uk\n",
    "- en.isabart.org\n",
    "- bechdeltest.com\n",
    "- npg.si.edu\n",
    "- www.guidetopharmacology.org\n",
    "- letterboxd.com\n",
    "- www.discogs.com\n",
    "- www.nytimes.com\n",
    "- vocab.getty.edu\n",
    "- www.isfdb.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_netloc_aggs = [\n",
    "    'witches.shca.ed.ac.uk','en.isabart.org','bechdeltest.com','npg.si.edu','www.guidetopharmacology.org',\n",
    "    'letterboxd.com','www.discogs.com','vocab.getty.edu','www.isfdb.org', 'www.npg.org.uk',\n",
    "    'art.nationalgalleries.org', 'www.tate.org.uk' ,'www.getty.edu', 'memory-beta.fandom.com', 'www.disease-ontology.org',\n",
    "    'artgallery.yale.edu', 'www.imdb.com', 'muckrack.com', 'live.dbpedia.org', 'dbpedia.org'\n",
    "]\n",
    "\n",
    "assert all([b in data_df_trim.netloc_agg.unique() for b in bad_netloc_aggs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_id</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>final_verbalisation</th>\n",
       "      <th>sampling_weight</th>\n",
       "      <th>final_url</th>\n",
       "      <th>netloc_agg</th>\n",
       "      <th>nlp_sentences</th>\n",
       "      <th>nlp_sentences_slide_2</th>\n",
       "      <th>nlp_sentences_scores</th>\n",
       "      <th>nlp_sentences_slide_2_scores</th>\n",
       "      <th>nlp_sentences_all_TOP_N</th>\n",
       "      <th>evidence_TE_prob_all_TOP_N</th>\n",
       "      <th>evidence_TE_prob_weighted_all_TOP_N</th>\n",
       "      <th>evidence_TE_labels_all_TOP_N</th>\n",
       "      <th>claim_TE_prob_weighted_sum_all_TOP_N</th>\n",
       "      <th>claim_TE_label_weighted_sum_all_TOP_N</th>\n",
       "      <th>claim_TE_label_malon_all_TOP_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390d6c6e68a32e11f8d7b0883cda0557db529fe6</td>\n",
       "      <td>Q5512528$81E8AD02-28AF-4AE3-8ACD-047C30B40B01</td>\n",
       "      <td>G V Raja died in Kullu Valley.</td>\n",
       "      <td>24749.538462</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=G._...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>[G. V. Raja., From Wikipedia, the free encyclo...</td>\n",
       "      <td>[G. V. Raja. From Wikipedia, the free encyclop...</td>\n",
       "      <td>[-0.9059305191, -0.9998914599000001, -0.999842...</td>\n",
       "      <td>[-0.9975231290000001, -0.9998638630000001, -0....</td>\n",
       "      <td>[{'sentence': 'He died in an air crash near Ku...</td>\n",
       "      <td>[[0.9708649516000001, 0.0061447304, 0.02299031...</td>\n",
       "      <td>[[0.9707732797, 0.0061441502, 0.0229881462], [...</td>\n",
       "      <td>[SUPPORTS, SUPPORTS, NOT ENOUGH INFO, NOT ENOU...</td>\n",
       "      <td>[1.9523134232000001, 0.0484717116, 2.5008661747]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49b9aec8e10815611ff0379a34d5fd7c3830566e</td>\n",
       "      <td>Q12149940$C9FE5F0C-78FD-4ECE-B1C2-16A763B8ED4E</td>\n",
       "      <td>Sebastian Sabol's religion is the Greek cathol...</td>\n",
       "      <td>24749.538462</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Seb...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>[Sebastian Sabol., From Wikipedia, the free en...</td>\n",
       "      <td>[Sebastian Sabol. From Wikipedia, the free enc...</td>\n",
       "      <td>[-0.9904284477, -0.9997430444000001, -0.999761...</td>\n",
       "      <td>[-0.9976871014, -0.9997031689, -0.999774158, -...</td>\n",
       "      <td>[{'sentence': 'Sebastian Stepan Sabol, O.S.B.M...</td>\n",
       "      <td>[[0.09385417400000001, 0.2980020344, 0.6081437...</td>\n",
       "      <td>[[0.0830457285, 0.2636834979, 0.5381085873], [...</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...</td>\n",
       "      <td>[0.1000050157, 0.4289829731, 0.8527254462]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ab3e9ada7246257ffbfb86fa90a54f25e45a704e</td>\n",
       "      <td>Q583556$2AD75C92-953E-47CB-93A6-D082674F4231</td>\n",
       "      <td>The average white band started in 1972.</td>\n",
       "      <td>24749.538462</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Ave...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>[Average White Band., From Wikipedia, the free...</td>\n",
       "      <td>[Average White Band. From Wikipedia, the free ...</td>\n",
       "      <td>[-0.9852669835000001, -0.9997751117, -0.999563...</td>\n",
       "      <td>[-0.9655147195, -0.9992546439000001, -0.999627...</td>\n",
       "      <td>[{'sentence': 'AWB was formed in early 1972 in...</td>\n",
       "      <td>[[0.9388607144000001, 0.0051775482, 0.05596167...</td>\n",
       "      <td>[[0.9337435365000001, 0.0051493281, 0.05565666...</td>\n",
       "      <td>[SUPPORTS, REFUTES, NOT ENOUGH INFO, NOT ENOUG...</td>\n",
       "      <td>[0.964186728, 0.8969765306, 0.25956323740000004]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76b04346ad57869d9e5ae1007ba8343d708ab6f9</td>\n",
       "      <td>Q7586053$AD344012-A522-4A8A-AF64-16F859E2EA9A</td>\n",
       "      <td>Yutaka Higuchi began work in 1983.</td>\n",
       "      <td>24749.538462</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Yut...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>[Yutaka Higuchi (musician) From Wikipedia, the...</td>\n",
       "      <td>[Yutaka Higuchi (musician) From Wikipedia, the...</td>\n",
       "      <td>[-0.9987268448000001, -0.9997874498, -0.999821...</td>\n",
       "      <td>[-0.9918303490000001, -0.9996852875000001, -0....</td>\n",
       "      <td>[{'sentence': 'Yutaka Higuchi (桶口 豊, Higuchi Y...</td>\n",
       "      <td>[[0.7398596406, 0.0883401632, 0.17180021110000...</td>\n",
       "      <td>[[0.7397438288, 0.088326335, 0.1717733145], [0...</td>\n",
       "      <td>[SUPPORTS, NOT ENOUGH INFO, REFUTES, NOT ENOUG...</td>\n",
       "      <td>[0.7442157269, 0.3141435385, 0.5475053787]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1ee71b39caf6df395c64b436fea4895692812d38</td>\n",
       "      <td>Q5289513$B0532490-8FEC-4744-9D74-61238B633898</td>\n",
       "      <td>Dolores Delirio started work in 1994.</td>\n",
       "      <td>24749.538462</td>\n",
       "      <td>https://en.wikipedia.org/w/index.php?title=Dol...</td>\n",
       "      <td>en.wikipedia.org</td>\n",
       "      <td>[Dolores Delirio., From Wikipedia, the free en...</td>\n",
       "      <td>[Dolores Delirio. From Wikipedia, the free enc...</td>\n",
       "      <td>[-0.49935883280000004, -0.9997779727, -0.99969...</td>\n",
       "      <td>[-0.9781491756, -0.9995777011, -0.9996224642, ...</td>\n",
       "      <td>[{'sentence': 'Dolores Delirio is a Peruvian r...</td>\n",
       "      <td>[[0.7100448608000001, 0.0457550883, 0.24419999...</td>\n",
       "      <td>[[0.7099757195, 0.0457506366, 0.24417622390000...</td>\n",
       "      <td>[SUPPORTS, NOT ENOUGH INFO, NOT ENOUGH INFO, N...</td>\n",
       "      <td>[0.7102969885, 0.0495032594, 0.2685163021]</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>e004706cba4821386dc31a86faeb680e8fff4ae7</td>\n",
       "      <td>Q58480207$B8B66EE1-CD20-4426-846C-4B37DC5A55A4</td>\n",
       "      <td>Laila Majnu is in black and white.</td>\n",
       "      <td>76.384615</td>\n",
       "      <td>https://indiancine.ma/ATB</td>\n",
       "      <td>indiancine.ma</td>\n",
       "      <td>[Laila Majnu (1931) Director: J.J. Madan; Cast...</td>\n",
       "      <td>[Laila Majnu (1931) Director: J.J. Madan; Cast...</td>\n",
       "      <td>[-0.23192951080000002, -0.9998129010000001, -0...</td>\n",
       "      <td>[-0.41434443, -0.9998908639, -0.9998814464, -0...</td>\n",
       "      <td>[{'sentence': 'Vanadevi (Ramakant Gharekhan) 1...</td>\n",
       "      <td>[[0.014269447000000001, 0.0084052524, 0.977325...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>c714ca621d1fe1ca35f20e01a72587d23bdd8e1e</td>\n",
       "      <td>Q58485125$0B087946-63A0-4D69-BAEA-AD52DFBE2C6F</td>\n",
       "      <td>Navjeevan's original language is Hindi.</td>\n",
       "      <td>76.384615</td>\n",
       "      <td>https://indiancine.ma/BTW</td>\n",
       "      <td>indiancine.ma</td>\n",
       "      <td>[Navjeevan (1935) Director: M. Bhavnani., Indi...</td>\n",
       "      <td>[Navjeevan (1935) Director: M. Bhavnani. India...</td>\n",
       "      <td>[-0.9962592721, -0.9998548031000001, -0.999881...</td>\n",
       "      <td>[-0.9990227818, -0.9999102354, -0.9998908043, ...</td>\n",
       "      <td>[{'sentence': 'Navjeevan M. Bhavnani Year: 193...</td>\n",
       "      <td>[[0.0557921417, 0.0390320905, 0.90517580510000...</td>\n",
       "      <td>[[0.0111095207, 0.0077722026, 0.1802416742]]</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...</td>\n",
       "      <td>[0.0111095207, 0.0077722026, 0.1802416742]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>52968f905e88f2484e3cb43e6ab4e76f120bc3c3</td>\n",
       "      <td>Q58485081$AF144153-1EF3-4494-BBCC-90742E594482</td>\n",
       "      <td>Bengali is the original language of Mantra Sha...</td>\n",
       "      <td>76.384615</td>\n",
       "      <td>https://indiancine.ma/BTD</td>\n",
       "      <td>indiancine.ma</td>\n",
       "      <td>[Mantra Shakti (1935) Director: Satu Sen. Indi...</td>\n",
       "      <td>[Mantra Shakti (1935) Director: Satu Sen. Indi...</td>\n",
       "      <td>[-0.9853174686, -0.9998137355000001, -0.999884...</td>\n",
       "      <td>[-0.9987406731, -0.9998266101000001, -0.999742...</td>\n",
       "      <td>[{'sentence': 'Mantra Shakti Satu Sen Year: 19...</td>\n",
       "      <td>[[0.2910126448, 0.0131028006, 0.6958845854], [...</td>\n",
       "      <td>[[0.0641431138, 0.0028880343, 0.15338236090000...</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...</td>\n",
       "      <td>[0.0642338172, 0.0029714017, 0.1621499956]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>8bb93e520f7ca8208f24078434b8f5f2ba8b520b</td>\n",
       "      <td>Q58484448$9611AF89-1BD2-4D49-9CBB-E3EC52C7765D</td>\n",
       "      <td>The original language of Gul Sanobar is Hindi.</td>\n",
       "      <td>76.384615</td>\n",
       "      <td>https://indiancine.ma/BKF</td>\n",
       "      <td>indiancine.ma</td>\n",
       "      <td>[Gul Sanobar (1934) Director: Homi Master; Wri...</td>\n",
       "      <td>[Gul Sanobar (1934) Director: Homi Master; Wri...</td>\n",
       "      <td>[-0.9783033729, -0.9661140442, -0.6003271341, ...</td>\n",
       "      <td>[-0.8527330756, -0.47588691120000004, -0.30289...</td>\n",
       "      <td>[{'sentence': 'Made. several B films in Hindi ...</td>\n",
       "      <td>[[0.1824070811, 0.0066584139, 0.81093454360000...</td>\n",
       "      <td>[[0.0983626321, 0.0035905356, 0.4372947216], [...</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, SUPPORTS, N...</td>\n",
       "      <td>[0.1575524956, 0.0082641244, 0.7491290569]</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>6e6204011de4f4176362a6f4067486a94fc6ac3e</td>\n",
       "      <td>Q58435606$82576BDF-9365-462C-B1F8-A6FF6B8FBE9D</td>\n",
       "      <td>Jai Bhawani is in black and white.</td>\n",
       "      <td>76.384615</td>\n",
       "      <td>https://indiancine.ma/YI</td>\n",
       "      <td>indiancine.ma</td>\n",
       "      <td>[Jai Bhawani (1928) Director: K.P. Bhave., Ind...</td>\n",
       "      <td>[Jai Bhawani (1928) Director: K.P. Bhave. Indi...</td>\n",
       "      <td>[-0.9492717981000001, -0.9998480082000001, -0....</td>\n",
       "      <td>[-0.9842366576, -0.9999023676000001, -0.999898...</td>\n",
       "      <td>[{'sentence': 'Heer Ranjha (Fatma Begum) 1928 ...</td>\n",
       "      <td>[[0.1017738879, 0.0090509988, 0.88917511700000...</td>\n",
       "      <td>[[0.006671467800000001, 0.0005933098, 0.058287...</td>\n",
       "      <td>[NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...</td>\n",
       "      <td>[0.006671467800000001, 0.0005933098, 0.0582870...</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>409 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 reference_id  \\\n",
       "0    390d6c6e68a32e11f8d7b0883cda0557db529fe6   \n",
       "1    49b9aec8e10815611ff0379a34d5fd7c3830566e   \n",
       "2    ab3e9ada7246257ffbfb86fa90a54f25e45a704e   \n",
       "3    76b04346ad57869d9e5ae1007ba8343d708ab6f9   \n",
       "4    1ee71b39caf6df395c64b436fea4895692812d38   \n",
       "..                                        ...   \n",
       "404  e004706cba4821386dc31a86faeb680e8fff4ae7   \n",
       "405  c714ca621d1fe1ca35f20e01a72587d23bdd8e1e   \n",
       "406  52968f905e88f2484e3cb43e6ab4e76f120bc3c3   \n",
       "407  8bb93e520f7ca8208f24078434b8f5f2ba8b520b   \n",
       "408  6e6204011de4f4176362a6f4067486a94fc6ac3e   \n",
       "\n",
       "                                           claim_id  \\\n",
       "0     Q5512528$81E8AD02-28AF-4AE3-8ACD-047C30B40B01   \n",
       "1    Q12149940$C9FE5F0C-78FD-4ECE-B1C2-16A763B8ED4E   \n",
       "2      Q583556$2AD75C92-953E-47CB-93A6-D082674F4231   \n",
       "3     Q7586053$AD344012-A522-4A8A-AF64-16F859E2EA9A   \n",
       "4     Q5289513$B0532490-8FEC-4744-9D74-61238B633898   \n",
       "..                                              ...   \n",
       "404  Q58480207$B8B66EE1-CD20-4426-846C-4B37DC5A55A4   \n",
       "405  Q58485125$0B087946-63A0-4D69-BAEA-AD52DFBE2C6F   \n",
       "406  Q58485081$AF144153-1EF3-4494-BBCC-90742E594482   \n",
       "407  Q58484448$9611AF89-1BD2-4D49-9CBB-E3EC52C7765D   \n",
       "408  Q58435606$82576BDF-9365-462C-B1F8-A6FF6B8FBE9D   \n",
       "\n",
       "                                   final_verbalisation  sampling_weight  \\\n",
       "0                       G V Raja died in Kullu Valley.     24749.538462   \n",
       "1    Sebastian Sabol's religion is the Greek cathol...     24749.538462   \n",
       "2              The average white band started in 1972.     24749.538462   \n",
       "3                   Yutaka Higuchi began work in 1983.     24749.538462   \n",
       "4                Dolores Delirio started work in 1994.     24749.538462   \n",
       "..                                                 ...              ...   \n",
       "404                 Laila Majnu is in black and white.        76.384615   \n",
       "405            Navjeevan's original language is Hindi.        76.384615   \n",
       "406  Bengali is the original language of Mantra Sha...        76.384615   \n",
       "407     The original language of Gul Sanobar is Hindi.        76.384615   \n",
       "408                 Jai Bhawani is in black and white.        76.384615   \n",
       "\n",
       "                                             final_url        netloc_agg  \\\n",
       "0    https://en.wikipedia.org/w/index.php?title=G._...  en.wikipedia.org   \n",
       "1    https://en.wikipedia.org/w/index.php?title=Seb...  en.wikipedia.org   \n",
       "2    https://en.wikipedia.org/w/index.php?title=Ave...  en.wikipedia.org   \n",
       "3    https://en.wikipedia.org/w/index.php?title=Yut...  en.wikipedia.org   \n",
       "4    https://en.wikipedia.org/w/index.php?title=Dol...  en.wikipedia.org   \n",
       "..                                                 ...               ...   \n",
       "404                          https://indiancine.ma/ATB     indiancine.ma   \n",
       "405                          https://indiancine.ma/BTW     indiancine.ma   \n",
       "406                          https://indiancine.ma/BTD     indiancine.ma   \n",
       "407                          https://indiancine.ma/BKF     indiancine.ma   \n",
       "408                           https://indiancine.ma/YI     indiancine.ma   \n",
       "\n",
       "                                         nlp_sentences  \\\n",
       "0    [G. V. Raja., From Wikipedia, the free encyclo...   \n",
       "1    [Sebastian Sabol., From Wikipedia, the free en...   \n",
       "2    [Average White Band., From Wikipedia, the free...   \n",
       "3    [Yutaka Higuchi (musician) From Wikipedia, the...   \n",
       "4    [Dolores Delirio., From Wikipedia, the free en...   \n",
       "..                                                 ...   \n",
       "404  [Laila Majnu (1931) Director: J.J. Madan; Cast...   \n",
       "405  [Navjeevan (1935) Director: M. Bhavnani., Indi...   \n",
       "406  [Mantra Shakti (1935) Director: Satu Sen. Indi...   \n",
       "407  [Gul Sanobar (1934) Director: Homi Master; Wri...   \n",
       "408  [Jai Bhawani (1928) Director: K.P. Bhave., Ind...   \n",
       "\n",
       "                                 nlp_sentences_slide_2  \\\n",
       "0    [G. V. Raja. From Wikipedia, the free encyclop...   \n",
       "1    [Sebastian Sabol. From Wikipedia, the free enc...   \n",
       "2    [Average White Band. From Wikipedia, the free ...   \n",
       "3    [Yutaka Higuchi (musician) From Wikipedia, the...   \n",
       "4    [Dolores Delirio. From Wikipedia, the free enc...   \n",
       "..                                                 ...   \n",
       "404  [Laila Majnu (1931) Director: J.J. Madan; Cast...   \n",
       "405  [Navjeevan (1935) Director: M. Bhavnani. India...   \n",
       "406  [Mantra Shakti (1935) Director: Satu Sen. Indi...   \n",
       "407  [Gul Sanobar (1934) Director: Homi Master; Wri...   \n",
       "408  [Jai Bhawani (1928) Director: K.P. Bhave. Indi...   \n",
       "\n",
       "                                  nlp_sentences_scores  \\\n",
       "0    [-0.9059305191, -0.9998914599000001, -0.999842...   \n",
       "1    [-0.9904284477, -0.9997430444000001, -0.999761...   \n",
       "2    [-0.9852669835000001, -0.9997751117, -0.999563...   \n",
       "3    [-0.9987268448000001, -0.9997874498, -0.999821...   \n",
       "4    [-0.49935883280000004, -0.9997779727, -0.99969...   \n",
       "..                                                 ...   \n",
       "404  [-0.23192951080000002, -0.9998129010000001, -0...   \n",
       "405  [-0.9962592721, -0.9998548031000001, -0.999881...   \n",
       "406  [-0.9853174686, -0.9998137355000001, -0.999884...   \n",
       "407  [-0.9783033729, -0.9661140442, -0.6003271341, ...   \n",
       "408  [-0.9492717981000001, -0.9998480082000001, -0....   \n",
       "\n",
       "                          nlp_sentences_slide_2_scores  \\\n",
       "0    [-0.9975231290000001, -0.9998638630000001, -0....   \n",
       "1    [-0.9976871014, -0.9997031689, -0.999774158, -...   \n",
       "2    [-0.9655147195, -0.9992546439000001, -0.999627...   \n",
       "3    [-0.9918303490000001, -0.9996852875000001, -0....   \n",
       "4    [-0.9781491756, -0.9995777011, -0.9996224642, ...   \n",
       "..                                                 ...   \n",
       "404  [-0.41434443, -0.9998908639, -0.9998814464, -0...   \n",
       "405  [-0.9990227818, -0.9999102354, -0.9998908043, ...   \n",
       "406  [-0.9987406731, -0.9998266101000001, -0.999742...   \n",
       "407  [-0.8527330756, -0.47588691120000004, -0.30289...   \n",
       "408  [-0.9842366576, -0.9999023676000001, -0.999898...   \n",
       "\n",
       "                               nlp_sentences_all_TOP_N  \\\n",
       "0    [{'sentence': 'He died in an air crash near Ku...   \n",
       "1    [{'sentence': 'Sebastian Stepan Sabol, O.S.B.M...   \n",
       "2    [{'sentence': 'AWB was formed in early 1972 in...   \n",
       "3    [{'sentence': 'Yutaka Higuchi (桶口 豊, Higuchi Y...   \n",
       "4    [{'sentence': 'Dolores Delirio is a Peruvian r...   \n",
       "..                                                 ...   \n",
       "404  [{'sentence': 'Vanadevi (Ramakant Gharekhan) 1...   \n",
       "405  [{'sentence': 'Navjeevan M. Bhavnani Year: 193...   \n",
       "406  [{'sentence': 'Mantra Shakti Satu Sen Year: 19...   \n",
       "407  [{'sentence': 'Made. several B films in Hindi ...   \n",
       "408  [{'sentence': 'Heer Ranjha (Fatma Begum) 1928 ...   \n",
       "\n",
       "                            evidence_TE_prob_all_TOP_N  \\\n",
       "0    [[0.9708649516000001, 0.0061447304, 0.02299031...   \n",
       "1    [[0.09385417400000001, 0.2980020344, 0.6081437...   \n",
       "2    [[0.9388607144000001, 0.0051775482, 0.05596167...   \n",
       "3    [[0.7398596406, 0.0883401632, 0.17180021110000...   \n",
       "4    [[0.7100448608000001, 0.0457550883, 0.24419999...   \n",
       "..                                                 ...   \n",
       "404  [[0.014269447000000001, 0.0084052524, 0.977325...   \n",
       "405  [[0.0557921417, 0.0390320905, 0.90517580510000...   \n",
       "406  [[0.2910126448, 0.0131028006, 0.6958845854], [...   \n",
       "407  [[0.1824070811, 0.0066584139, 0.81093454360000...   \n",
       "408  [[0.1017738879, 0.0090509988, 0.88917511700000...   \n",
       "\n",
       "                   evidence_TE_prob_weighted_all_TOP_N  \\\n",
       "0    [[0.9707732797, 0.0061441502, 0.0229881462], [...   \n",
       "1    [[0.0830457285, 0.2636834979, 0.5381085873], [...   \n",
       "2    [[0.9337435365000001, 0.0051493281, 0.05565666...   \n",
       "3    [[0.7397438288, 0.088326335, 0.1717733145], [0...   \n",
       "4    [[0.7099757195, 0.0457506366, 0.24417622390000...   \n",
       "..                                                 ...   \n",
       "404                                                 []   \n",
       "405       [[0.0111095207, 0.0077722026, 0.1802416742]]   \n",
       "406  [[0.0641431138, 0.0028880343, 0.15338236090000...   \n",
       "407  [[0.0983626321, 0.0035905356, 0.4372947216], [...   \n",
       "408  [[0.006671467800000001, 0.0005933098, 0.058287...   \n",
       "\n",
       "                          evidence_TE_labels_all_TOP_N  \\\n",
       "0    [SUPPORTS, SUPPORTS, NOT ENOUGH INFO, NOT ENOU...   \n",
       "1    [NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...   \n",
       "2    [SUPPORTS, REFUTES, NOT ENOUGH INFO, NOT ENOUG...   \n",
       "3    [SUPPORTS, NOT ENOUGH INFO, REFUTES, NOT ENOUG...   \n",
       "4    [SUPPORTS, NOT ENOUGH INFO, NOT ENOUGH INFO, N...   \n",
       "..                                                 ...   \n",
       "404  [NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...   \n",
       "405  [NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...   \n",
       "406  [NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...   \n",
       "407  [NOT ENOUGH INFO, NOT ENOUGH INFO, SUPPORTS, N...   \n",
       "408  [NOT ENOUGH INFO, NOT ENOUGH INFO, NOT ENOUGH ...   \n",
       "\n",
       "                  claim_TE_prob_weighted_sum_all_TOP_N  \\\n",
       "0     [1.9523134232000001, 0.0484717116, 2.5008661747]   \n",
       "1           [0.1000050157, 0.4289829731, 0.8527254462]   \n",
       "2     [0.964186728, 0.8969765306, 0.25956323740000004]   \n",
       "3           [0.7442157269, 0.3141435385, 0.5475053787]   \n",
       "4           [0.7102969885, 0.0495032594, 0.2685163021]   \n",
       "..                                                 ...   \n",
       "404                                          [0, 0, 0]   \n",
       "405         [0.0111095207, 0.0077722026, 0.1802416742]   \n",
       "406         [0.0642338172, 0.0029714017, 0.1621499956]   \n",
       "407         [0.1575524956, 0.0082641244, 0.7491290569]   \n",
       "408  [0.006671467800000001, 0.0005933098, 0.0582870...   \n",
       "\n",
       "    claim_TE_label_weighted_sum_all_TOP_N claim_TE_label_malon_all_TOP_N  \n",
       "0                         NOT ENOUGH INFO                       SUPPORTS  \n",
       "1                         NOT ENOUGH INFO                       SUPPORTS  \n",
       "2                                SUPPORTS                       SUPPORTS  \n",
       "3                                SUPPORTS                       SUPPORTS  \n",
       "4                                SUPPORTS                       SUPPORTS  \n",
       "..                                    ...                            ...  \n",
       "404                       NOT ENOUGH INFO                NOT ENOUGH INFO  \n",
       "405                       NOT ENOUGH INFO                NOT ENOUGH INFO  \n",
       "406                       NOT ENOUGH INFO                NOT ENOUGH INFO  \n",
       "407                       NOT ENOUGH INFO                       SUPPORTS  \n",
       "408                       NOT ENOUGH INFO                NOT ENOUGH INFO  \n",
       "\n",
       "[409 rows x 17 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_trim_good = data_df_trim[~data_df_trim.netloc_agg.isin(bad_netloc_aggs)].reset_index(drop=True)\n",
    "data_df_trim_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now the costs are:\n",
    "((data_df_trim_good.shape[0]*5)-45)/4*5*0.5*1.2\n",
    "# Which are much more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whereas for the *multiple evidence* task paying $1 each, they are:\n",
    "(data_df_trim_good.shape[0]-45)/4*5*1*1.2\n",
    "\n",
    "# For a total of 1787.5, which is A-ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = data_df_trim_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PILOT\n",
    "\n",
    "If we select 20 entries that means:\n",
    "\n",
    "- For the **single evidence** task, we will have 20\\*5 = 100 subtasks\n",
    "    - Taking 8 as golden data means (100-8)/4 = **23** tasks.\n",
    "    - Giving \\\\$0.5 per task, giving them for 5 annotators, with 20\\% to Amazon, means 23\\*5\\*0.5\\*1.2 = \\\\$69.\n",
    "- For the **multiple evidence** task, we will have 20 subtasks\n",
    "    - Taking 4 as golden data means (20-4)/4 = **4** tasks.\n",
    "    - Giving \\\\$1 per task, giving them for 5 annotators, with 20\\% to Amazon, means 4\\*5\\*1\\*1.2 = \\\\$24.\n",
    "    \n",
    "Total Pilot cost = $93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 172 to 77\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           20 non-null     object \n",
      " 1   claim_id                               20 non-null     object \n",
      " 2   final_verbalisation                    20 non-null     object \n",
      " 3   sampling_weight                        20 non-null     float64\n",
      " 4   final_url                              20 non-null     object \n",
      " 5   netloc_agg                             20 non-null     object \n",
      " 6   nlp_sentences                          20 non-null     object \n",
      " 7   nlp_sentences_slide_2                  20 non-null     object \n",
      " 8   nlp_sentences_scores                   20 non-null     object \n",
      " 9   nlp_sentences_slide_2_scores           20 non-null     object \n",
      " 10  nlp_sentences_all_TOP_N                20 non-null     object \n",
      " 11  evidence_TE_prob_all_TOP_N             20 non-null     object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    20 non-null     object \n",
      " 13  evidence_TE_labels_all_TOP_N           20 non-null     object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   20 non-null     object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  20 non-null     object \n",
      " 16  claim_TE_label_malon_all_TOP_N         20 non-null     object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "task_data_pilot= task_data.sample(20, random_state=42)\n",
    "task_data_pilot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9714a104ff11447eaf9a930df6885178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " {'claim_id': 'Q64566934$EA9C53AA-1E8D-4D0D-814E-0FB00E7AA3EE',\n",
       "  'reference_id': 'c142c251fcb563f07dabef11a283b4fd171f1eb6',\n",
       "  'evidence_id': 0,\n",
       "  'sentence_id': '14',\n",
       "  'score': 0.29895803330000004,\n",
       "  'affirmation': 'Pandallur Hss is located in India.',\n",
       "  'pre_evidence': 'PANDALLUR HSS was established in 1979 and it is managed by the Pvt. Aided. It is located in Rural area.',\n",
       "  'evidence': 'It is located in MANJERI block of MALAPPURAM district of Kerala.',\n",
       "  'post_evidence': \"The school consists of Grades from 8 to 12. The school is Co-educational and it doesn't have an attached pre-primary section.\",\n",
       "  'reference_url': 'https://schools.org.in/malappuram/32050601215/',\n",
       "  'g_id': -1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_SINGLE_pilot = convert_df_to_json_set_SINGLE(task_data_pilot)\n",
    "len(jsonsets_SINGLE_pilot), jsonsets_SINGLE_pilot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c066d6258c254e00a368824cae8a5ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " {'claim_id': 'Q100562234$6222DFB3-EC74-4C9D-B02D-BB70AFB981B9',\n",
       "  'reference_id': '5e9bc44ce8b4ae04f57985660ab5f880cee53315',\n",
       "  'affirmation': 'Merchant Shipping (Grain) Rules, 1953 has a citation of S.I. No. 348/1953.',\n",
       "  'reference_url': 'https://www.irishstatutebook.ie/eli/1953/si/348/made/en/print',\n",
       "  'g_id': -1,\n",
       "  'evidence_list': [{'evidence_id': 0,\n",
       "    'sentence_id': '47;48',\n",
       "    'score': 0.9998652339,\n",
       "    'pre_evidence': 'Advanced Search Cuardach Casta. Home Baile. Statutory Instruments Ionstraimí Reachtúla. 1953. S.I. No.',\n",
       "    'evidence': '348/1953- Merchant Shipping (Grain) Rules, 1953. S.I.',\n",
       "    'post_evidence': 'No. 348/1953- Merchant Shipping (Grain) Rules, 1953. View SI Amharc ar an IR. Amendments Leasuithe. S.I.'},\n",
       "   {'evidence_id': 4,\n",
       "    'sentence_id': '61;62',\n",
       "    'score': 0.1842070073,\n",
       "    'pre_evidence': \"These Rules supersede all regulations made under section' 453 of the Merchant Shipping Act, 1894, before the coming into operation of these Rules which said regulations are accordingly hereby revoked. 3.\",\n",
       "    'evidence': 'These Rules shall come into operation on the 19th day of November, 1953. 4. Every precaution set forth in the Schedule to these Rules is hereby prescribed as being, subject to the provisions of the said Schedule, a precaution to be treated for the purposes of subsections (1) and (2) of the said section 39 as a necessary or reasonable precaution to prevent grain from shifting, in the case of ships of the following classes:—. (a) ships which are loaded with grain within any port in the State; (b) ships which, having been loaded with grain outside the State, enter any port in the State so laden.',\n",
       "    'post_evidence': '5. Where these Rules require that a particular fitting, appliance or apparatus, or type thereof, shall be fitted or carried in a ship or that any particular provision shall be made, the Minister for Industry and Commerce may allow any other fitting, appliance or apparatus, or type thereof, to be fitted or carried, or any other provision to be made in that ship if he is satisfied that such other fitting, appliance or apparatus, or type thereof, or provision is at least as effective as that required by these Rules.'},\n",
       "   {'evidence_id': 2,\n",
       "    'sentence_id': '57',\n",
       "    'score': 0.9918883443000001,\n",
       "    'pre_evidence': 'I, SEÁN F. LEMASS, Minister for Industry and Commerce, in exercise of the powers conferred upon me by subsection (3) of section 39 of the Merchant Shipping (Safety Convention) Act, 1952 (No. 29 of 1952), hereby make the following Rules:—. 1.',\n",
       "    'evidence': 'These Rules may be cited as the Merchant Shipping (Grain) Rules, 1953.',\n",
       "    'post_evidence': \"2. These Rules supersede all regulations made under section' 453 of the Merchant Shipping Act, 1894, before the coming into operation of these Rules which said regulations are accordingly hereby revoked.\"},\n",
       "   {'evidence_id': 1,\n",
       "    'sentence_id': '49;50',\n",
       "    'score': 0.9998426437000001,\n",
       "    'pre_evidence': 'Statutory Instruments Ionstraimí Reachtúla. 1953. S.I. No. 348/1953- Merchant Shipping (Grain) Rules, 1953. S.I.',\n",
       "    'evidence': 'No. 348/1953- Merchant Shipping (Grain) Rules, 1953.',\n",
       "    'post_evidence': 'View SI Amharc ar an IR. Amendments Leasuithe. S.I. No. 348 of 1953. MERCHANT SHIPPING (GRAIN) RULES, 1953.'},\n",
       "   {'evidence_id': 3,\n",
       "    'sentence_id': '54;55',\n",
       "    'score': 0.9158077836,\n",
       "    'pre_evidence': '348/1953- Merchant Shipping (Grain) Rules, 1953. View SI Amharc ar an IR. Amendments Leasuithe. S.I.',\n",
       "    'evidence': 'No. 348 of 1953. MERCHANT SHIPPING (GRAIN) RULES, 1953.',\n",
       "    'post_evidence': 'I, SEÁN F. LEMASS, Minister for Industry and Commerce, in exercise of the powers conferred upon me by subsection (3) of section 39 of the Merchant Shipping (Safety Convention) Act, 1952 (No. 29 of 1952), hereby make the following Rules:—. 1.'}]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_MULTIPLE_pilot = convert_df_to_json_set_MULTIPLE(task_data_pilot)\n",
    "len(jsonsets_MULTIPLE_pilot), jsonsets_MULTIPLE_pilot[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Gold Standard\n",
    "\n",
    "We now split the json sets into two, one with golden standards and one with references to be judged.\n",
    "\n",
    "Here is we calculate an appropriate number of gold standards:\n",
    "- Let X be the maximum expected number of tasks any singular worker can complete;\n",
    "- Let Y be the number of gold standard references we have annotated;\n",
    "- Let Z be the number of combinations of 2 gold standard references that we can take from the Y gold standard references in our gold set, regardless of pairing order, without repeating.\n",
    "- Let P be the probability of a worker doing X tasks and not finding a repeated pair of golden standard references, taken from the set of Z gold standard pairs.\n",
    "\n",
    "So we calculate:\n",
    "- Z = Y*(Y-1)/2\n",
    "- P = Z!/((Z^X)*(Z-X)!)\n",
    "\n",
    "So, we set X = 15 (The mean number of tasks per worker for previous experiments has been 4, with standard deviation of 10.5, so if it follows a normal distribution, we get 85% of workers here).\n",
    "\n",
    "According to the calculation below, we see that **Y = 45** gives us near 90% of chances of not having repeated gold standard, so that is how many gold standards we aim at annotating for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x191ccb6ac10>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "import matplotlib.pyplot as plt\n",
    "def fac(x):\n",
    "    return Decimal(math.factorial(x))\n",
    "x = 15\n",
    "ys = list(range(15,100))\n",
    "zs = [y*(y-1)/2 for y in ys]\n",
    "ps = [fac(z)/(Decimal(z**x)*fac(z-x)) for z in zs]\n",
    "plt.plot(ys,ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota_SINGLE_pilot = 8 #should be 45 for the campaign\n",
    "golden_standard_quota_MULTIPLE_pilot = 4\n",
    "\n",
    "jsonsets_SINGLE_pilot_shuffle = random.Random(42).sample(jsonsets_SINGLE_pilot, len(jsonsets_SINGLE_pilot))\n",
    "#with open('../data/pilot/single_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_SINGLE_pilot_shuffle[:golden_standard_quota_SINGLE_pilot],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "with open('../data/pilot/single.json','w+',encoding='utf8') as f:\n",
    "    json.dump(\n",
    "        jsonsets_SINGLE_pilot_shuffle[golden_standard_quota_SINGLE_pilot:],\n",
    "        f, indent=2,ensure_ascii=False\n",
    "    )   \n",
    "    \n",
    "jsonsets_MULTIPLE_pilot_shuffle = random.Random(42).sample(jsonsets_MULTIPLE_pilot, len(jsonsets_MULTIPLE_pilot))\n",
    "#with open('../data/pilot/multiple_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_MULTIPLE_pilot_shuffle[:golden_standard_quota_MULTIPLE_pilot],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "with open('../data/pilot/multiple.json','w+',encoding='utf8') as f:\n",
    "    json.dump(\n",
    "        jsonsets_MULTIPLE_pilot_shuffle[golden_standard_quota_MULTIPLE_pilot:],\n",
    "        f, indent=2,ensure_ascii=False\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must now annotate the golden standard by doing the following:\n",
    "For each gd reference in the gd json set, go to the \"gd\" field, which should be like this:\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": -1\n",
    "}\n",
    "```\n",
    "\n",
    "and change it to something of the format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": [list of allowed values]\n",
    "}\n",
    "```\n",
    "where by allowed values we mean any of the following which could apply to this case:\n",
    "- 0: Supports\n",
    "- 1: Refutes\n",
    "- 2: Neither\n",
    "- 3: Not Sure\n",
    "\n",
    "If any golden data case is ambiguous, exchange it for a non-gd case and **save to a x_gd_2.json** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Task Sets\n",
    "\n",
    "**Make sure you have annotated the GD before proceeding here**\n",
    "\n",
    "**Make sure you have also filtered the non-GD for API-verifiable examples**\n",
    "\n",
    "Now we take 4 non_gd references and 2 gd references and pack them into task sets of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [SINGLE]\n",
      "Gd len: 8\n",
      "Non-Gd len: 92\n",
      "Generated 23 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [6, 6, 6, 6, 6, 5, 6, 5]\n",
      "\n",
      "Estimated costs: $69.0\n"
     ]
    }
   ],
   "source": [
    "cost_single_task = 0.5\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 6 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "print('Generating Task Sets [SINGLE]')\n",
    "with open('../data/pilot/single_gd.json','r',encoding='utf8') as f:\n",
    "    single_gd = json.load(f)\n",
    "    print('Gd len:',len(single_gd))\n",
    "with open('../data/pilot/single.json','r',encoding='utf8') as f:\n",
    "    single = json.load(f)\n",
    "    print('Non-Gd len:',len(single))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(single, single_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_single_task*1.2}')\n",
    "\n",
    "with open('../data/pilot/single_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [MULTIPLE]\n",
      "Gd len: 4\n",
      "Non-Gd len: 16\n",
      "Generated 4 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 2, 2, 2]\n",
      "\n",
      "Estimated costs: $24.0\n"
     ]
    }
   ],
   "source": [
    "cost_single_task = 1\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 2 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "\n",
    "print('Generating Task Sets [MULTIPLE]')\n",
    "with open('../data/pilot/multiple_gd.json','r',encoding='utf8') as f:\n",
    "    verbalisations_gd = json.load(f)\n",
    "    print('Gd len:',len(verbalisations_gd))\n",
    "with open('../data/pilot/multiple.json','r',encoding='utf8') as f:\n",
    "    verbalisations = json.load(f)\n",
    "    print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_single_task*1.2}')\n",
    "\n",
    "with open('../data/pilot/multiple_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total cost is $93'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Total cost is ${69+24}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMPAIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6651 entries, 0 to 6650\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   entity_id                6651 non-null   object \n",
      " 1   claim_id                 6651 non-null   object \n",
      " 2   rank                     6651 non-null   object \n",
      " 3   property_id              6651 non-null   object \n",
      " 4   datatype                 6651 non-null   object \n",
      " 5   datavalue                6651 non-null   object \n",
      " 6   sampling_weight_vb       6651 non-null   object \n",
      " 7   sampling_weight          6651 non-null   float64\n",
      " 8   entity_label             6651 non-null   object \n",
      " 9   property_label           6651 non-null   object \n",
      " 10  object_label             6651 non-null   object \n",
      " 11  theme_entity_id          6651 non-null   object \n",
      " 12  theme_entity_label       6651 non-null   object \n",
      " 13  entity_desc              6651 non-null   object \n",
      " 14  property_desc            6651 non-null   object \n",
      " 15  object_desc              6651 non-null   object \n",
      " 16  entity_alias             6651 non-null   object \n",
      " 17  property_alias           6651 non-null   object \n",
      " 18  object_alias             6651 non-null   object \n",
      " 19  entity_label_lan         6651 non-null   object \n",
      " 20  property_label_lan       6651 non-null   object \n",
      " 21  object_label_lan         6651 non-null   object \n",
      " 22  entity_desc_lan          6651 non-null   object \n",
      " 23  property_desc_lan        6651 non-null   object \n",
      " 24  object_desc_lan          6651 non-null   object \n",
      " 25  entity_alias_lan         6651 non-null   object \n",
      " 26  property_alias_lan       6651 non-null   object \n",
      " 27  object_alias_lan         6651 non-null   object \n",
      " 28  verbalisation            6651 non-null   object \n",
      " 29  processed_verbalisation  6651 non-null   object \n",
      " 30  unk_count                6651 non-null   int64  \n",
      " 31  property_and_theme_id    6651 non-null   object \n",
      " 32  campaign_group           6651 non-null   int64  \n",
      "dtypes: float64(1), int64(2), object(30)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data/campaign/campaign_sampled_df_verbalised_english.csv')\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     159\n",
       "1     159\n",
       "4     159\n",
       "8     159\n",
       "12    159\n",
       "13    159\n",
       "9     159\n",
       "11    159\n",
       "7     159\n",
       "3     159\n",
       "5     159\n",
       "2     159\n",
       "6     159\n",
       "10    159\n",
       "14    159\n",
       "32    158\n",
       "40    158\n",
       "36    158\n",
       "21    158\n",
       "28    158\n",
       "24    158\n",
       "20    158\n",
       "16    158\n",
       "17    158\n",
       "39    158\n",
       "25    158\n",
       "34    158\n",
       "31    158\n",
       "27    158\n",
       "23    158\n",
       "19    158\n",
       "15    158\n",
       "38    158\n",
       "30    158\n",
       "29    158\n",
       "26    158\n",
       "22    158\n",
       "18    158\n",
       "41    158\n",
       "35    158\n",
       "33    158\n",
       "37    158\n",
       "Name: campaign_group, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.campaign_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Artificial Golden Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_remaining = data_df[data_df.campaign_group >= 3].reset_index(drop=True) # Taken from not campaigns groups 0,1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 45 golden data points with poor fluency\n",
    "\n",
    "Select these apart and then manually alter them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=42).reset_index(drop=True))\n",
    "\n",
    "for jsonset in jsonsets:\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2],\n",
    "      \"adequacy\": [0,1,2]\n",
    "    }  \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_FLUENCY'\n",
    "    jsonset['verbalised_claim'] = jsonset['verbalised_claim'] + ' $ALTER THIS'\n",
    "    \n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_fluency.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 45 golden data points with poor adequacy\n",
    "\n",
    "Triples are paired with random verbalisations from other triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r",
      "2.22 %                \r",
      "4.44 %                \r",
      "6.67 %                \r",
      "8.89 %                \r",
      "11.11 %                \r",
      "13.33 %                \r",
      "15.56 %                \r",
      "17.78 %                \r",
      "20.0 %                \r",
      "22.22 %                \r",
      "24.44 %                \r",
      "26.67 %                \r",
      "28.89 %                \r",
      "31.11 %                \r",
      "33.33 %                \r",
      "35.56 %                \r",
      "37.78 %                \r",
      "40.0 %                \r",
      "42.22 %                \r",
      "44.44 %                \r",
      "46.67 %                \r",
      "48.89 %                \r",
      "51.11 %                \r",
      "53.33 %                \r",
      "55.56 %                \r",
      "57.78 %                \r",
      "60.0 %                \r",
      "62.22 %                \r",
      "64.44 %                \r",
      "66.67 %                \r",
      "68.89 %                \r",
      "71.11 %                \r",
      "73.33 %                \r",
      "75.56 %                \r",
      "77.78 %                \r",
      "80.0 %                \r",
      "82.22 %                \r",
      "84.44 %                \r",
      "86.67 %                \r",
      "88.89 %                \r",
      "91.11 %                \r",
      "93.33 %                \r",
      "95.56 %                \r",
      "97.78 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=24783).reset_index(drop=True))\n",
    "jsonsets_2 = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=1847).reset_index(drop=True))\n",
    "\n",
    "for i, jsonset in enumerate(jsonsets):\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2,3,4,5],\n",
    "      \"adequacy\": [1,2]\n",
    "    }  \n",
    "    \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_ADEQUACY'    \n",
    "    jsonset['verbalised_claim'] = jsonsets_2[i]['verbalised_claim']\n",
    "\n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_adequacy.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are first converting campaign batch 1! That is campaign groups 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_batch_1 = data_df[data_df.campaign_group < 3].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21 %                \r",
      "0.42 %                \r",
      "0.63 %                \r",
      "0.84 %                \r",
      "1.05 %                \r",
      "1.26 %                \r",
      "1.47 %                \r",
      "1.68 %                \r",
      "1.89 %                \r",
      "2.1 %                \r",
      "2.31 %                \r",
      "2.52 %                \r",
      "2.73 %                \r",
      "2.94 %                \r",
      "3.14 %                \r",
      "3.35 %                \r",
      "3.56 %                \r",
      "3.77 %                \r",
      "3.98 %                \r",
      "4.19 %                \r",
      "4.4 %                \r",
      "4.61 %                \r",
      "4.82 %                \r",
      "5.03 %                \r",
      "5.24 %                \r",
      "5.45 %                \r",
      "5.66 %                \r",
      "5.87 %                \r",
      "6.08 %                \r",
      "6.29 %                \r",
      "6.5 %                \r",
      "6.71 %                \r",
      "6.92 %                \r",
      "7.13 %                \r",
      "7.34 %                \r",
      "7.55 %                \r",
      "7.76 %                \r",
      "7.97 %                \r",
      "8.18 %                \r",
      "8.39 %                \r",
      "8.6 %                \r",
      "8.81 %                \r",
      "9.01 %                \r",
      "9.22 %                \r",
      "9.43 %                \r",
      "9.64 %                \r",
      "9.85 %                \r",
      "10.06 %                \r",
      "10.27 %                \r",
      "10.48 %                \r",
      "10.69 %                \r",
      "10.9 %                \r",
      "11.11 %                \r",
      "11.32 %                \r",
      "11.53 %                \r",
      "11.74 %                \r",
      "11.95 %                \r",
      "12.16 %                \r",
      "12.37 %                \r",
      "12.58 %                \r",
      "12.79 %                \r",
      "13.0 %                \r",
      "13.21 %                \r",
      "13.42 %                \r",
      "13.63 %                \r",
      "13.84 %                \r",
      "14.05 %                \r",
      "14.26 %                \r",
      "14.47 %                \r",
      "14.68 %                \r",
      "14.88 %                \r",
      "15.09 %                \r",
      "15.3 %                \r",
      "15.51 %                \r",
      "15.72 %                \r",
      "15.93 %                \r",
      "16.14 %                \r",
      "16.35 %                \r",
      "16.56 %                \r",
      "16.77 %                \r",
      "16.98 %                \r",
      "17.19 %                \r",
      "17.4 %                \r",
      "17.61 %                \r",
      "17.82 %                \r",
      "18.03 %                \r",
      "18.24 %                \r",
      "18.45 %                \r",
      "18.66 %                \r",
      "18.87 %                \r",
      "19.08 %                \r",
      "19.29 %                \r",
      "19.5 %                \r",
      "19.71 %                \r",
      "19.92 %                \r",
      "20.13 %                \r",
      "20.34 %                \r",
      "20.55 %                \r",
      "20.75 %                \r",
      "20.96 %                \r",
      "21.17 %                \r",
      "21.38 %                \r",
      "21.59 %                \r",
      "21.8 %                \r",
      "22.01 %                \r",
      "22.22 %                \r",
      "22.43 %                \r",
      "22.64 %                \r",
      "22.85 %                \r",
      "23.06 %                \r",
      "23.27 %                \r",
      "23.48 %                \r",
      "23.69 %                \r",
      "23.9 %                \r",
      "24.11 %                \r",
      "24.32 %                \r",
      "24.53 %                \r",
      "24.74 %                \r",
      "24.95 %                \r",
      "25.16 %                \r",
      "25.37 %                \r",
      "25.58 %                \r",
      "25.79 %                \r",
      "26.0 %                \r",
      "26.21 %                \r",
      "26.42 %                \r",
      "26.62 %                \r",
      "26.83 %                \r",
      "27.04 %                \r",
      "27.25 %                \r",
      "27.46 %                \r",
      "27.67 %                \r",
      "27.88 %                \r",
      "28.09 %                \r",
      "28.3 %                \r",
      "28.51 %                \r",
      "28.72 %                \r",
      "28.93 %                \r",
      "29.14 %                \r",
      "29.35 %                \r",
      "29.56 %                \r",
      "29.77 %                \r",
      "29.98 %                \r",
      "30.19 %                \r",
      "30.4 %                \r",
      "30.61 %                \r",
      "30.82 %                \r",
      "31.03 %                \r",
      "31.24 %                \r",
      "31.45 %                \r",
      "31.66 %                \r",
      "31.87 %                \r",
      "32.08 %                \r",
      "32.29 %                \r",
      "32.49 %                \r",
      "32.7 %                \r",
      "32.91 %                \r",
      "33.12 %                \r",
      "33.33 %                \r",
      "33.54 %                \r",
      "33.75 %                \r",
      "33.96 %                \r",
      "34.17 %                \r",
      "34.38 %                \r",
      "34.59 %                \r",
      "34.8 %                \r",
      "35.01 %                \r",
      "35.22 %                \r",
      "35.43 %                \r",
      "35.64 %                \r",
      "35.85 %                \r",
      "36.06 %                \r",
      "36.27 %                \r",
      "36.48 %                \r",
      "36.69 %                \r",
      "36.9 %                \r",
      "37.11 %                \r",
      "37.32 %                \r",
      "37.53 %                \r",
      "37.74 %                \r",
      "37.95 %                \r",
      "38.16 %                \r",
      "38.36 %                \r",
      "38.57 %                \r",
      "38.78 %                \r",
      "38.99 %                \r",
      "39.2 %                \r",
      "39.41 %                \r",
      "39.62 %                \r",
      "39.83 %                \r",
      "40.04 %                \r",
      "40.25 %                \r",
      "40.46 %                \r",
      "40.67 %                \r",
      "40.88 %                \r",
      "41.09 %                \r",
      "41.3 %                \r",
      "41.51 %                \r",
      "41.72 %                \r",
      "41.93 %                \r",
      "42.14 %                \r",
      "42.35 %                \r",
      "42.56 %                \r",
      "42.77 %                \r",
      "42.98 %                \r",
      "43.19 %                \r",
      "43.4 %                \r",
      "43.61 %                \r",
      "43.82 %                \r",
      "44.03 %                \r",
      "44.23 %                \r",
      "44.44 %                \r",
      "44.65 %                \r",
      "44.86 %                \r",
      "45.07 %                \r",
      "45.28 %                \r",
      "45.49 %                \r",
      "45.7 %                \r",
      "45.91 %                \r",
      "46.12 %                \r",
      "46.33 %                \r",
      "46.54 %                \r",
      "46.75 %                \r",
      "46.96 %                \r",
      "47.17 %                \r",
      "47.38 %                \r",
      "47.59 %                \r",
      "47.8 %                \r",
      "48.01 %                \r",
      "48.22 %                \r",
      "48.43 %                \r",
      "48.64 %                \r",
      "48.85 %                \r",
      "49.06 %                \r",
      "49.27 %                \r",
      "49.48 %                \r",
      "49.69 %                \r",
      "49.9 %                \r",
      "50.1 %                \r",
      "50.31 %                \r",
      "50.52 %                \r",
      "50.73 %                \r",
      "50.94 %                \r",
      "51.15 %                \r",
      "51.36 %                \r",
      "51.57 %                \r",
      "51.78 %                \r",
      "51.99 %                \r",
      "52.2 %                \r",
      "52.41 %                \r",
      "52.62 %                \r",
      "52.83 %                \r",
      "53.04 %                \r",
      "53.25 %                \r",
      "53.46 %                \r",
      "53.67 %                \r",
      "53.88 %                \r",
      "54.09 %                \r",
      "54.3 %                \r",
      "54.51 %                \r",
      "54.72 %                \r",
      "54.93 %                \r",
      "55.14 %                \r",
      "55.35 %                \r",
      "55.56 %                \r",
      "55.77 %                \r",
      "55.97 %                \r",
      "56.18 %                \r",
      "56.39 %                \r",
      "56.6 %                \r",
      "56.81 %                \r",
      "57.02 %                \r",
      "57.23 %                \r",
      "57.44 %                \r",
      "57.65 %                \r",
      "57.86 %                \r",
      "58.07 %                \r",
      "58.28 %                \r",
      "58.49 %                \r",
      "58.7 %                \r",
      "58.91 %                \r",
      "59.12 %                \r",
      "59.33 %                \r",
      "59.54 %                \r",
      "59.75 %                \r",
      "59.96 %                \r",
      "60.17 %                \r",
      "60.38 %                \r",
      "60.59 %                \r",
      "60.8 %                \r",
      "61.01 %                \r",
      "61.22 %                \r",
      "61.43 %                \r",
      "61.64 %                \r",
      "61.84 %                \r",
      "62.05 %                \r",
      "62.26 %                \r",
      "62.47 %                \r",
      "62.68 %                \r",
      "62.89 %                \r",
      "63.1 %                \r",
      "63.31 %                \r",
      "63.52 %                \r",
      "63.73 %                \r",
      "63.94 %                \r",
      "64.15 %                \r",
      "64.36 %                \r",
      "64.57 %                \r",
      "64.78 %                \r",
      "64.99 %                \r",
      "65.2 %                \r",
      "65.41 %                \r",
      "65.62 %                \r",
      "65.83 %                \r",
      "66.04 %                \r",
      "66.25 %                \r",
      "66.46 %                \r",
      "66.67 %                \r",
      "66.88 %                \r",
      "67.09 %                \r",
      "67.3 %                \r",
      "67.51 %                \r",
      "67.71 %                \r",
      "67.92 %                \r",
      "68.13 %                \r",
      "68.34 %                \r",
      "68.55 %                \r",
      "68.76 %                \r",
      "68.97 %                \r",
      "69.18 %                \r",
      "69.39 %                \r",
      "69.6 %                \r",
      "69.81 %                \r",
      "70.02 %                \r",
      "70.23 %                \r",
      "70.44 %                \r",
      "70.65 %                \r",
      "70.86 %                \r",
      "71.07 %                \r",
      "71.28 %                \r",
      "71.49 %                \r",
      "71.7 %                \r",
      "71.91 %                \r",
      "72.12 %                \r",
      "72.33 %                \r",
      "72.54 %                \r",
      "72.75 %                \r",
      "72.96 %                \r",
      "73.17 %                \r",
      "73.38 %                \r",
      "73.58 %                \r",
      "73.79 %                \r",
      "74.0 %                \r",
      "74.21 %                \r",
      "74.42 %                \r",
      "74.63 %                \r",
      "74.84 %                \r",
      "75.05 %                \r",
      "75.26 %                \r",
      "75.47 %                \r",
      "75.68 %                \r",
      "75.89 %                \r",
      "76.1 %                \r",
      "76.31 %                \r",
      "76.52 %                \r",
      "76.73 %                \r",
      "76.94 %                \r",
      "77.15 %                \r",
      "77.36 %                \r",
      "77.57 %                \r",
      "77.78 %                \r",
      "77.99 %                \r",
      "78.2 %                \r",
      "78.41 %                \r",
      "78.62 %                \r",
      "78.83 %                \r",
      "79.04 %                \r",
      "79.25 %                \r",
      "79.45 %                \r",
      "79.66 %                \r",
      "79.87 %                \r",
      "80.08 %                \r",
      "80.29 %                \r",
      "80.5 %                \r",
      "80.71 %                \r",
      "80.92 %                \r",
      "81.13 %                \r",
      "81.34 %                \r",
      "81.55 %                \r",
      "81.76 %                \r",
      "81.97 %                \r",
      "82.18 %                \r",
      "82.39 %                \r",
      "82.6 %                \r",
      "82.81 %                \r",
      "83.02 %                \r",
      "83.23 %                \r",
      "83.44 %                \r",
      "83.65 %                \r",
      "83.86 %                \r",
      "84.07 %                \r",
      "84.28 %                \r",
      "84.49 %                \r",
      "84.7 %                \r",
      "84.91 %                \r",
      "85.12 %                \r",
      "85.32 %                \r",
      "85.53 %                \r",
      "85.74 %                \r",
      "85.95 %                \r",
      "86.16 %                \r",
      "86.37 %                \r",
      "86.58 %                \r",
      "86.79 %                \r",
      "87.0 %                \r",
      "87.21 %                \r",
      "87.42 %                \r",
      "87.63 %                \r",
      "87.84 %                \r",
      "88.05 %                \r",
      "88.26 %                \r",
      "88.47 %                \r",
      "88.68 %                \r",
      "88.89 %                \r",
      "89.1 %                \r",
      "89.31 %                \r",
      "89.52 %                \r",
      "89.73 %                \r",
      "89.94 %                \r",
      "90.15 %                \r",
      "90.36 %                \r",
      "90.57 %                \r",
      "90.78"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " %                \r",
      "90.99 %                \r",
      "91.19 %                \r",
      "91.4 %                \r",
      "91.61 %                \r",
      "91.82 %                \r",
      "92.03 %                \r",
      "92.24 %                \r",
      "92.45 %                \r",
      "92.66 %                \r",
      "92.87 %                \r",
      "93.08 %                \r",
      "93.29 %                \r",
      "93.5 %                \r",
      "93.71 %                \r",
      "93.92 %                \r",
      "94.13 %                \r",
      "94.34 %                \r",
      "94.55 %                \r",
      "94.76 %                \r",
      "94.97 %                \r",
      "95.18 %                \r",
      "95.39 %                \r",
      "95.6 %                \r",
      "95.81 %                \r",
      "96.02 %                \r",
      "96.23 %                \r",
      "96.44 %                \r",
      "96.65 %                \r",
      "96.86 %                \r",
      "97.06 %                \r",
      "97.27 %                \r",
      "97.48 %                \r",
      "97.69 %                \r",
      "97.9 %                \r",
      "98.11 %                \r",
      "98.32 %                \r",
      "98.53 %                \r",
      "98.74 %                \r",
      "98.95 %                \r",
      "99.16 %                \r",
      "99.37 %                \r",
      "99.58 %                \r",
      "99.79 %                \r",
      "100.0 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota = 45\n",
    "\n",
    "jsonsets_shuffle = random.Random(42).sample(jsonsets, len(jsonsets))\n",
    "with open('data/campaign/batch_1/verbalisations_gd.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets_shuffle[:golden_standard_quota], f, indent=2,ensure_ascii=False)\n",
    "with open('data/campaign/batch_1/verbalisations.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets_shuffle[golden_standard_quota:], f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 432\n",
      "Generated 108 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 3, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 4, 3, 4, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 4, 2, 4, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2]\n",
      "Generating Task Sets\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 432\n",
      "Generated 108 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 4, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 5, 2, 2, 2, 2, 3, 4, 2, 2, 3, 2, 4, 2, 2, 2, 2, 3, 4, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 3, 3, 2, 2, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "for task_type in ['fluency','adequacy']:\n",
    "\n",
    "    print(f'Generating Task Sets for {task_type}')\n",
    "    with open('data/campaign/batch_1/verbalisations_gd.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd = json.load(f)\n",
    "        print('Gd len:',len(verbalisations_gd))\n",
    "    with open(f'data/campaign/batch_1/verbalisations_gd_plus_{task_type}.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd_plus = json.load(f)\n",
    "        print('Gd plus len:',len(verbalisations_gd))\n",
    "    with open('data/campaign/batch_1/verbalisations.json','r',encoding='utf8') as f:\n",
    "        verbalisations = json.load(f)\n",
    "        print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "    maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "    maxSingleInstances_gd = 2\n",
    "\n",
    "    ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd + verbalisations_gd_plus, maxSingleInstances, maxSingleInstances_gd)\n",
    "    print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "    print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "\n",
    "\n",
    "    with open(f'./data/campaign/batch_1/TaskSets_{task_type}.json','w+',encoding='utf8') as f:\n",
    "        json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are now converting campaign batch 2! That is campaign groups 10,15,20,25,30,35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cgs = [10,15,20,25,30,35]\n",
    "data_df_batch_2 = data_df[data_df.campaign_group.apply(lambda x : x in selected_cgs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 %                 %                \r"
     ]
    }
   ],
   "source": [
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL USE THE SAME GOLD DATA AS FOR BATCH 1, NO NEED TO ANNOTATE MORE IF THOSE WORKED\n",
    "\n",
    "jsonsets_shuffle = random.Random(42).sample(jsonsets, len(jsonsets))\n",
    "with open('data/campaign/batch_2/verbalisations.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets_shuffle, f, indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets for fluency\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 949\n",
      "Generated 238 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [5, 5, 5, 5, 5, 5, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5, 5, 8, 5, 5, 5, 5, 6, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 6, 5, 5, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 5, 5, 6, 5, 6, 5, 6, 5, 6, 5]\n",
      "Generating Task Sets for adequacy\n",
      "Gd len: 45\n",
      "Gd plus len: 45\n",
      "Non-Gd len: 949\n",
      "Generated 238 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [6, 5, 5, 6, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 6, 5, 5, 5, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 6, 5, 5, 6, 5, 6, 6, 5, 5, 5, 5, 6, 6, 5, 5, 6, 5, 5, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 5, 5, 7, 6, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "for task_type in ['fluency','adequacy']:\n",
    "\n",
    "    print(f'Generating Task Sets for {task_type}')\n",
    "    with open('data/campaign/batch_1/verbalisations_gd.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd = json.load(f)\n",
    "        print('Gd len:',len(verbalisations_gd))\n",
    "    with open(f'data/campaign/batch_1/verbalisations_gd_plus_{task_type}.json','r',encoding='utf8') as f:\n",
    "        verbalisations_gd_plus = json.load(f)\n",
    "        print('Gd plus len:',len(verbalisations_gd))\n",
    "    with open('data/campaign/batch_2/verbalisations.json','r',encoding='utf8') as f:\n",
    "        verbalisations = json.load(f)\n",
    "        print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "    maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "    maxSingleInstances_gd = 5\n",
    "\n",
    "    ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd + verbalisations_gd_plus, maxSingleInstances, maxSingleInstances_gd)\n",
    "    print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "    print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "\n",
    "\n",
    "    with open(f'./data/campaign/batch_2/TaskSets_{task_type}.json','w+',encoding='utf8') as f:\n",
    "        json.dump(ds,f,indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

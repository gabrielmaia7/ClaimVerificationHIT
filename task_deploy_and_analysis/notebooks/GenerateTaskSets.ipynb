{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import traceback\n",
    "import math\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_json_set_SINGLE(df):\n",
    "    microtask_jsons = []\n",
    "    padding = 100\n",
    "    try:\n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            for idy, evidence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "                \n",
    "                # Let us retrieve the sections before and after the evidence. 100 characters to each side will do.\n",
    "                all_sentences = row['nlp_sentences']    \n",
    "                \n",
    "                evidence_idx = evidence['sentence_id']\n",
    "                if ';' in evidence_idx:\n",
    "                    before_idx = int(evidence_idx.split(';')[0])-1\n",
    "                    after_idx = int(evidence_idx.split(';')[1])+1\n",
    "                else:                    \n",
    "                    before_idx = int(evidence_idx)-1\n",
    "                    after_idx = int(evidence_idx)+1\n",
    "                    \n",
    "                pre_evidence = []\n",
    "                for i in range(before_idx,-1,-1):\n",
    "                    pre_evidence.insert(0, all_sentences[i])\n",
    "                    if len(' '.join(pre_evidence))>=100:\n",
    "                        break\n",
    "                pre_evidence = ' '.join(pre_evidence)\n",
    "                \n",
    "                post_evidence = []\n",
    "                for i in range(after_idx,len(all_sentences)):\n",
    "                    post_evidence.append(all_sentences[i])\n",
    "                    if len(' '.join(post_evidence))>=100:\n",
    "                        break\n",
    "                post_evidence = ' '.join(post_evidence)\n",
    "                \n",
    "                assert ' '.join([pre_evidence, evidence['sentence'], post_evidence]).strip() in ' '.join(all_sentences)\n",
    "                \n",
    "                microtask_json = {\n",
    "                    \"claim_id\": row['claim_id'],\n",
    "                    \"reference_id\": row['reference_id'],\n",
    "                    \"evidence_id\": idy,\n",
    "                    \"sentence_id\": evidence_idx,\n",
    "                    \"score\": evidence['score'],\n",
    "                    \"affirmation\": row['final_verbalisation'],\n",
    "                    \"pre_evidence\": pre_evidence,\n",
    "                    \"evidence\": evidence['sentence'],\n",
    "                    \"post_evidence\": post_evidence,\n",
    "                    \"reference_url\": row['final_url'],\n",
    "                    \"g_id\": -1\n",
    "                }\n",
    "                microtask_jsons.append(microtask_json)\n",
    "                #raise ValueError\n",
    "                \n",
    "        return microtask_jsons\n",
    "    except Exception:\n",
    "        print(' '.join([pre_evidence, evidence['sentence'], post_evidence]))\n",
    "        #print(pre_evidence)\n",
    "        #print('--')\n",
    "        #print(evidence['sentence'])\n",
    "        #print('--')\n",
    "        #print(post_evidence)\n",
    "        print('--')\n",
    "        print(' '.join(all_sentences))\n",
    "        #print(row)\n",
    "        #traceback.print_exc()\n",
    "        #pdb.set_trace()\n",
    "        raise\n",
    "        \n",
    "def convert_df_to_json_set_MULTIPLE(df):\n",
    "    microtask_jsons = []\n",
    "    padding = 100\n",
    "    try:\n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            evidence_list = []\n",
    "            all_sentences = row['nlp_sentences']\n",
    "            \n",
    "            for idy, evidence in enumerate(row['nlp_sentences_all_TOP_N']):\n",
    "                              \n",
    "                evidence_idx = evidence['sentence_id']\n",
    "                if ';' in evidence_idx:\n",
    "                    before_idx = int(evidence_idx.split(';')[0])-1\n",
    "                    after_idx = int(evidence_idx.split(';')[1])+1\n",
    "                else:                    \n",
    "                    before_idx = int(evidence_idx)-1\n",
    "                    after_idx = int(evidence_idx)+1\n",
    "                    \n",
    "                pre_evidence = []\n",
    "                for i in range(before_idx,-1,-1):\n",
    "                    pre_evidence.insert(0, all_sentences[i])\n",
    "                    if len(' '.join(pre_evidence))>=100:\n",
    "                        break\n",
    "                pre_evidence = ' '.join(pre_evidence)\n",
    "                \n",
    "                post_evidence = []\n",
    "                for i in range(after_idx,len(all_sentences)):\n",
    "                    post_evidence.append(all_sentences[i])\n",
    "                    if len(' '.join(post_evidence))>=100:\n",
    "                        break\n",
    "                post_evidence = ' '.join(post_evidence)\n",
    "                \n",
    "                assert ' '.join([pre_evidence, evidence['sentence'], post_evidence]).strip() in ' '.join(all_sentences)\n",
    "                \n",
    "                evidence_list.append({\n",
    "                    \"evidence_id\": idy,\n",
    "                    \"sentence_id\": evidence_idx,\n",
    "                    \"score\": evidence['score'],\n",
    "                    \"pre_evidence\": pre_evidence,\n",
    "                    \"evidence\": evidence['sentence'],\n",
    "                    \"post_evidence\": post_evidence,\n",
    "                })\n",
    "            evidence_list_shuffle = random.Random(\n",
    "                len(evidence_list[0]['evidence'])\n",
    "            ).sample(evidence_list, len(evidence_list))    \n",
    "            microtask_json = {\n",
    "                \"claim_id\": row['claim_id'],\n",
    "                \"reference_id\": row['reference_id'],\n",
    "                \"affirmation\": row['final_verbalisation'],\n",
    "                \"reference_url\": row['final_url'],\n",
    "                \"g_id\": -1,\n",
    "                \"evidence_list\": evidence_list_shuffle\n",
    "            }\n",
    "            microtask_jsons.append(microtask_json)\n",
    "                \n",
    "        return microtask_jsons\n",
    "    except Exception:\n",
    "        print(' '.join([pre_evidence, evidence['sentence'], post_evidence]))\n",
    "        #print(pre_evidence)\n",
    "        #print('--')\n",
    "        #print(evidence['sentence'])\n",
    "        #print('--')\n",
    "        #print(post_evidence)\n",
    "        print('--')\n",
    "        print(' '.join(all_sentences))\n",
    "        #print(row)\n",
    "        #traceback.print_exc()\n",
    "        #pdb.set_trace()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxSingleInstances = 1 #MAX TIMES ANY REFERENCE APPEARS AMONG THE TASK SETS\n",
    "#maxSingleInstances_gd = 2\n",
    "\n",
    "def getRandomTask(counter, n, non_gd_references, maxSingleInstances):\n",
    "    p = [max(maxSingleInstances-c,0.00001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(non_gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )        \n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = non_gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def getRandomGoldenTask(counter, n, gd_references, maxSingleInstances_gd):    \n",
    "    p = [max(maxSingleInstances_gd-c,0.00001) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(\n",
    "        a = list(range(len(gd_references))),\n",
    "        size=n,\n",
    "        replace=False,\n",
    "        p=p\n",
    "    )\n",
    "    task = []\n",
    "    for i in indexes:\n",
    "        reference = gd_references[i]\n",
    "        task.append((reference,i))\n",
    "    return task\n",
    "\n",
    "def generateTaskSet(counter, counter_gd, non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd, n=(4,2)):\n",
    "    '''\n",
    "    counter = a counter which keeps track of how many times each reference was retrieved\n",
    "    n = (x,y) where x = number of non_gd references and y = number of gd references\n",
    "    '''\n",
    "    taskSet = []\n",
    "    task = getRandomTask(counter, n[0], non_gd_references, maxSingleInstances)\n",
    "    taskSet += [r for (r,i) in task] # pairs reference,index are generated here, so that we can update the counter later\n",
    "    task_gd = getRandomGoldenTask(counter_gd, n[1], gd_references, maxSingleInstances_gd)\n",
    "    taskSet += [r for (r,i) in task_gd]\n",
    "    taskSet_shuffle = random.Random(42).sample(taskSet, len(taskSet))\n",
    "    return taskSet_shuffle, [i for (p,i) in task], [i for (p,i) in task_gd] # we return indices here to update counter\n",
    "\n",
    "\n",
    "def generateIDdTaskSets(non_gd_references, gd_references, maxSingleInstances, maxSingleInstances_gd):\n",
    "    counter = [0]*len(non_gd_references)\n",
    "    counter_gd = [0]*len(gd_references)\n",
    "    taskSets = []\n",
    "    while (any([c < maxSingleInstances for c in counter])):\n",
    "        taskSet, indexes, indexes_gd = generateTaskSet(counter, counter_gd, non_gd_references,\n",
    "                                                       gd_references, maxSingleInstances, maxSingleInstances_gd)\n",
    "        taskSetIDd = {\n",
    "            '_id': str(uuid.uuid4()),\n",
    "            'taskSet' : taskSet\n",
    "        }\n",
    "        taskSets.append(taskSetIDd)\n",
    "        for i in indexes:\n",
    "            counter[i] = counter[i] + 1\n",
    "        for i in indexes_gd:\n",
    "            counter_gd[i] = counter_gd[i] + 1\n",
    "    return taskSets, counter, counter_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 409 entries, 0 to 408\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           409 non-null    object \n",
      " 1   claim_id                               409 non-null    object \n",
      " 2   final_verbalisation                    409 non-null    object \n",
      " 3   sampling_weight                        409 non-null    float64\n",
      " 4   final_url                              409 non-null    object \n",
      " 5   netloc_agg                             409 non-null    object \n",
      " 6   nlp_sentences                          409 non-null    object \n",
      " 7   nlp_sentences_slide_2                  409 non-null    object \n",
      " 8   nlp_sentences_scores                   409 non-null    object \n",
      " 9   nlp_sentences_slide_2_scores           409 non-null    object \n",
      " 10  nlp_sentences_all_TOP_N                409 non-null    object \n",
      " 11  evidence_TE_prob_all_TOP_N             409 non-null    object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    409 non-null    object \n",
      " 13  evidence_TE_labels_all_TOP_N           409 non-null    object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   409 non-null    object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  409 non-null    object \n",
      " 16  claim_TE_label_malon_all_TOP_N         409 non-null    object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 54.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_json('../data/textual_entailment_df_2.json')\n",
    "data_df = data_df.drop([c for c in data_df.columns if 'TOP_N' in c and 'all_TOP_N' not in c], axis=1)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated cost of running **single evidence (T1)** tasks for all 409 verbalisations, paying 0.5 per task is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((data_df.shape[0]*5)-45)/4*5*0.5*1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas for the **multiple evidence (T2)** task paying $1 each, they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_df_trim_good.shape[0]-45)/4*5*1*1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a total of 1787.5, which is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data.to_json('../data/textual_entailment_df_filtered_for_crowdsourcing.json',orient='records',indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PILOT\n",
    "\n",
    "If we select 20 entries that means:\n",
    "\n",
    "- For the **single evidence** task, we will have 20\\*5 = 100 subtasks\n",
    "    - Taking 8 as golden data means (100-8)/4 = **23** tasks.\n",
    "    - Giving \\\\$0.5 per task, giving them for 5 annotators, with 20\\% to Amazon, means 23\\*5\\*0.5\\*1.2 = \\\\$69.\n",
    "- For the **multiple evidence** task, we will have 20 subtasks\n",
    "    - Taking 4 as golden data means (20-4)/4 = **4** tasks.\n",
    "    - Giving \\\\$1 per task, giving them for 5 annotators, with 20\\% to Amazon, means 4\\*5\\*1\\*1.2 = \\\\$24.\n",
    "    \n",
    "Total Pilot cost = $93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data = pd.read_json('../data/textual_entailment_df_filtered_for_crowdsourcing.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 172 to 77\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           20 non-null     object \n",
      " 1   claim_id                               20 non-null     object \n",
      " 2   final_verbalisation                    20 non-null     object \n",
      " 3   sampling_weight                        20 non-null     float64\n",
      " 4   final_url                              20 non-null     object \n",
      " 5   netloc_agg                             20 non-null     object \n",
      " 6   nlp_sentences                          20 non-null     object \n",
      " 7   nlp_sentences_slide_2                  20 non-null     object \n",
      " 8   nlp_sentences_scores                   20 non-null     object \n",
      " 9   nlp_sentences_slide_2_scores           20 non-null     object \n",
      " 10  nlp_sentences_all_TOP_N                20 non-null     object \n",
      " 11  evidence_TE_prob_all_TOP_N             20 non-null     object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    20 non-null     object \n",
      " 13  evidence_TE_labels_all_TOP_N           20 non-null     object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   20 non-null     object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  20 non-null     object \n",
      " 16  claim_TE_label_malon_all_TOP_N         20 non-null     object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 2.8+ KB\n"
     ]
    }
   ],
   "source": [
    "task_data_pilot= task_data.sample(20, random_state=42)\n",
    "task_data_pilot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bfe4f4d7cb4b1cbceb56646ab66797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " {'claim_id': 'Q64566934$EA9C53AA-1E8D-4D0D-814E-0FB00E7AA3EE',\n",
       "  'reference_id': 'c142c251fcb563f07dabef11a283b4fd171f1eb6',\n",
       "  'evidence_id': 0,\n",
       "  'sentence_id': '14',\n",
       "  'score': 0.29895803330000004,\n",
       "  'affirmation': 'Pandallur Hss is located in India.',\n",
       "  'pre_evidence': 'PANDALLUR HSS was established in 1979 and it is managed by the Pvt. Aided. It is located in Rural area.',\n",
       "  'evidence': 'It is located in MANJERI block of MALAPPURAM district of Kerala.',\n",
       "  'post_evidence': \"The school consists of Grades from 8 to 12. The school is Co-educational and it doesn't have an attached pre-primary section.\",\n",
       "  'reference_url': 'https://schools.org.in/malappuram/32050601215/',\n",
       "  'g_id': -1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_SINGLE_pilot = convert_df_to_json_set_SINGLE(task_data_pilot)\n",
    "len(jsonsets_SINGLE_pilot), jsonsets_SINGLE_pilot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c066d6258c254e00a368824cae8a5ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " {'claim_id': 'Q100562234$6222DFB3-EC74-4C9D-B02D-BB70AFB981B9',\n",
       "  'reference_id': '5e9bc44ce8b4ae04f57985660ab5f880cee53315',\n",
       "  'affirmation': 'Merchant Shipping (Grain) Rules, 1953 has a citation of S.I. No. 348/1953.',\n",
       "  'reference_url': 'https://www.irishstatutebook.ie/eli/1953/si/348/made/en/print',\n",
       "  'g_id': -1,\n",
       "  'evidence_list': [{'evidence_id': 0,\n",
       "    'sentence_id': '47;48',\n",
       "    'score': 0.9998652339,\n",
       "    'pre_evidence': 'Advanced Search Cuardach Casta. Home Baile. Statutory Instruments Ionstraimí Reachtúla. 1953. S.I. No.',\n",
       "    'evidence': '348/1953- Merchant Shipping (Grain) Rules, 1953. S.I.',\n",
       "    'post_evidence': 'No. 348/1953- Merchant Shipping (Grain) Rules, 1953. View SI Amharc ar an IR. Amendments Leasuithe. S.I.'},\n",
       "   {'evidence_id': 4,\n",
       "    'sentence_id': '61;62',\n",
       "    'score': 0.1842070073,\n",
       "    'pre_evidence': \"These Rules supersede all regulations made under section' 453 of the Merchant Shipping Act, 1894, before the coming into operation of these Rules which said regulations are accordingly hereby revoked. 3.\",\n",
       "    'evidence': 'These Rules shall come into operation on the 19th day of November, 1953. 4. Every precaution set forth in the Schedule to these Rules is hereby prescribed as being, subject to the provisions of the said Schedule, a precaution to be treated for the purposes of subsections (1) and (2) of the said section 39 as a necessary or reasonable precaution to prevent grain from shifting, in the case of ships of the following classes:—. (a) ships which are loaded with grain within any port in the State; (b) ships which, having been loaded with grain outside the State, enter any port in the State so laden.',\n",
       "    'post_evidence': '5. Where these Rules require that a particular fitting, appliance or apparatus, or type thereof, shall be fitted or carried in a ship or that any particular provision shall be made, the Minister for Industry and Commerce may allow any other fitting, appliance or apparatus, or type thereof, to be fitted or carried, or any other provision to be made in that ship if he is satisfied that such other fitting, appliance or apparatus, or type thereof, or provision is at least as effective as that required by these Rules.'},\n",
       "   {'evidence_id': 2,\n",
       "    'sentence_id': '57',\n",
       "    'score': 0.9918883443000001,\n",
       "    'pre_evidence': 'I, SEÁN F. LEMASS, Minister for Industry and Commerce, in exercise of the powers conferred upon me by subsection (3) of section 39 of the Merchant Shipping (Safety Convention) Act, 1952 (No. 29 of 1952), hereby make the following Rules:—. 1.',\n",
       "    'evidence': 'These Rules may be cited as the Merchant Shipping (Grain) Rules, 1953.',\n",
       "    'post_evidence': \"2. These Rules supersede all regulations made under section' 453 of the Merchant Shipping Act, 1894, before the coming into operation of these Rules which said regulations are accordingly hereby revoked.\"},\n",
       "   {'evidence_id': 1,\n",
       "    'sentence_id': '49;50',\n",
       "    'score': 0.9998426437000001,\n",
       "    'pre_evidence': 'Statutory Instruments Ionstraimí Reachtúla. 1953. S.I. No. 348/1953- Merchant Shipping (Grain) Rules, 1953. S.I.',\n",
       "    'evidence': 'No. 348/1953- Merchant Shipping (Grain) Rules, 1953.',\n",
       "    'post_evidence': 'View SI Amharc ar an IR. Amendments Leasuithe. S.I. No. 348 of 1953. MERCHANT SHIPPING (GRAIN) RULES, 1953.'},\n",
       "   {'evidence_id': 3,\n",
       "    'sentence_id': '54;55',\n",
       "    'score': 0.9158077836,\n",
       "    'pre_evidence': '348/1953- Merchant Shipping (Grain) Rules, 1953. View SI Amharc ar an IR. Amendments Leasuithe. S.I.',\n",
       "    'evidence': 'No. 348 of 1953. MERCHANT SHIPPING (GRAIN) RULES, 1953.',\n",
       "    'post_evidence': 'I, SEÁN F. LEMASS, Minister for Industry and Commerce, in exercise of the powers conferred upon me by subsection (3) of section 39 of the Merchant Shipping (Safety Convention) Act, 1952 (No. 29 of 1952), hereby make the following Rules:—. 1.'}]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_MULTIPLE_pilot = convert_df_to_json_set_MULTIPLE(task_data_pilot)\n",
    "len(jsonsets_MULTIPLE_pilot), jsonsets_MULTIPLE_pilot[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Gold Standard\n",
    "\n",
    "We now split the json sets into two, one with golden standards and one with references to be judged.\n",
    "\n",
    "Here is we calculate an appropriate number of gold standards:\n",
    "- Let X be the maximum expected number of tasks any singular worker can complete;\n",
    "- Let Y be the number of gold standard references we have annotated;\n",
    "- Let Z be the number of combinations of 2 gold standard references that we can take from the Y gold standard references in our gold set, regardless of pairing order, without repeating.\n",
    "- Let P be the probability of a worker doing X tasks and not finding a repeated pair of golden standard references, taken from the set of Z gold standard pairs.\n",
    "\n",
    "So we calculate:\n",
    "- Z = Y*(Y-1)/2\n",
    "- P = Z!/((Z^X)*(Z-X)!)\n",
    "\n",
    "So, we set X = 15 (The mean number of tasks per worker for previous experiments has been 4, with standard deviation of 10.5, so if it follows a normal distribution, we get 85% of workers here).\n",
    "\n",
    "According to the calculation below, we see that **Y = 45** gives us near 90% of chances of not having repeated gold standard, so that is how many gold standards we aim at annotating for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x191ccb6ac10>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "import matplotlib.pyplot as plt\n",
    "def fac(x):\n",
    "    return Decimal(math.factorial(x))\n",
    "x = 15\n",
    "ys = list(range(15,100))\n",
    "zs = [y*(y-1)/2 for y in ys]\n",
    "ps = [fac(z)/(Decimal(z**x)*fac(z-x)) for z in zs]\n",
    "plt.plot(ys,ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota_SINGLE_pilot = 8 #should be 45 for the campaign\n",
    "golden_standard_quota_MULTIPLE_pilot = 4\n",
    "\n",
    "jsonsets_SINGLE_pilot_shuffle = random.Random(42).sample(jsonsets_SINGLE_pilot, len(jsonsets_SINGLE_pilot))\n",
    "#with open('../data/pilot/single_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_SINGLE_pilot_shuffle[:golden_standard_quota_SINGLE_pilot],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "with open('../data/pilot/single.json','w+',encoding='utf8') as f:\n",
    "    json.dump(\n",
    "        jsonsets_SINGLE_pilot_shuffle[golden_standard_quota_SINGLE_pilot:],\n",
    "        f, indent=2,ensure_ascii=False\n",
    "    )   \n",
    "    \n",
    "jsonsets_MULTIPLE_pilot_shuffle = random.Random(42).sample(jsonsets_MULTIPLE_pilot, len(jsonsets_MULTIPLE_pilot))\n",
    "#with open('../data/pilot/multiple_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_MULTIPLE_pilot_shuffle[:golden_standard_quota_MULTIPLE_pilot],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "with open('../data/pilot/multiple.json','w+',encoding='utf8') as f:\n",
    "    json.dump(\n",
    "        jsonsets_MULTIPLE_pilot_shuffle[golden_standard_quota_MULTIPLE_pilot:],\n",
    "        f, indent=2,ensure_ascii=False\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must now annotate the golden standard by doing the following:\n",
    "For each gd reference in the gd json set, go to the \"gd\" field, which should be like this:\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": -1\n",
    "}\n",
    "```\n",
    "\n",
    "and change it to something of the format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": [list of allowed values]\n",
    "}\n",
    "```\n",
    "where by allowed values we mean any of the following which could apply to this case:\n",
    "- 0: Supports\n",
    "- 1: Refutes\n",
    "- 2: Neither\n",
    "- 3: Not Sure\n",
    "\n",
    "If any golden data case is ambiguous, exchange it for a non-gd case and **save to a x_gd_2.json** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Task Sets\n",
    "\n",
    "**Make sure you have annotated the GD before proceeding here**\n",
    "\n",
    "**Make sure you have also filtered the non-GD for API-verifiable examples**\n",
    "\n",
    "Now we take 4 non_gd references and 2 gd references and pack them into task sets of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [SINGLE]\n",
      "Gd len: 8\n",
      "Non-Gd len: 92\n",
      "Generated 23 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [6, 6, 6, 6, 6, 5, 6, 5]\n",
      "\n",
      "Estimated costs: $69.0\n"
     ]
    }
   ],
   "source": [
    "cost_single_task = 0.5\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 6 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "print('Generating Task Sets [SINGLE]')\n",
    "with open('../data/pilot/single_gd.json','r',encoding='utf8') as f:\n",
    "    single_gd = json.load(f)\n",
    "    print('Gd len:',len(single_gd))\n",
    "with open('../data/pilot/single.json','r',encoding='utf8') as f:\n",
    "    single = json.load(f)\n",
    "    print('Non-Gd len:',len(single))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(single, single_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_single_task*1.2}')\n",
    "\n",
    "with open('../data/pilot/single_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [MULTIPLE]\n",
      "Gd len: 4\n",
      "Non-Gd len: 16\n",
      "Generated 4 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [2, 2, 2, 2]\n",
      "\n",
      "Estimated costs: $24.0\n"
     ]
    }
   ],
   "source": [
    "cost_single_task = 1\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 2 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "\n",
    "print('Generating Task Sets [MULTIPLE]')\n",
    "with open('../data/pilot/multiple_gd.json','r',encoding='utf8') as f:\n",
    "    verbalisations_gd = json.load(f)\n",
    "    print('Gd len:',len(verbalisations_gd))\n",
    "with open('../data/pilot/multiple.json','r',encoding='utf8') as f:\n",
    "    verbalisations = json.load(f)\n",
    "    print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_single_task*1.2}')\n",
    "\n",
    "with open('../data/pilot/multiple_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total cost is $93'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Total cost is ${69+24}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAMPAIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 409 entries, 0 to 408\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   reference_id                           409 non-null    object \n",
      " 1   claim_id                               409 non-null    object \n",
      " 2   final_verbalisation                    409 non-null    object \n",
      " 3   sampling_weight                        409 non-null    float64\n",
      " 4   final_url                              409 non-null    object \n",
      " 5   netloc_agg                             409 non-null    object \n",
      " 6   nlp_sentences                          409 non-null    object \n",
      " 7   nlp_sentences_slide_2                  409 non-null    object \n",
      " 8   nlp_sentences_scores                   409 non-null    object \n",
      " 9   nlp_sentences_slide_2_scores           409 non-null    object \n",
      " 10  nlp_sentences_all_TOP_N                409 non-null    object \n",
      " 11  evidence_TE_prob_all_TOP_N             409 non-null    object \n",
      " 12  evidence_TE_prob_weighted_all_TOP_N    409 non-null    object \n",
      " 13  evidence_TE_labels_all_TOP_N           409 non-null    object \n",
      " 14  claim_TE_prob_weighted_sum_all_TOP_N   409 non-null    object \n",
      " 15  claim_TE_label_weighted_sum_all_TOP_N  409 non-null    object \n",
      " 16  claim_TE_label_malon_all_TOP_N         409 non-null    object \n",
      "dtypes: float64(1), object(16)\n",
      "memory usage: 54.4+ KB\n"
     ]
    }
   ],
   "source": [
    "task_data = pd.read_json('../data/textual_entailment_df_filtered_for_crowdsourcing.json')\n",
    "task_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9321a5e787934b84867668ca63ade18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2045,\n",
       " {'claim_id': 'Q5512528$81E8AD02-28AF-4AE3-8ACD-047C30B40B01',\n",
       "  'reference_id': '390d6c6e68a32e11f8d7b0883cda0557db529fe6',\n",
       "  'evidence_id': 0,\n",
       "  'sentence_id': '28;29',\n",
       "  'score': 0.9999055862,\n",
       "  'affirmation': 'G V Raja died in Kullu Valley.',\n",
       "  'pre_evidence': 'G.V Raja was also the President of Tourism Promotion Council of Kerala. He was the main architect in developing Kovalam as an international tourist spot.',\n",
       "  'evidence': 'He died in an air crash near Kullu (Kulu) Valley on April 30, 1971. Sports journalists, historians, experts and sportsmen consider him as the Father of Sports and Tourism in Kerala.',\n",
       "  'post_evidence': 'G.V Raja\\'s birth anniversary, 13 October, is observed as \"Kerala Sports Day\". Contents. 1 Background and early life.',\n",
       "  'reference_url': 'https://en.wikipedia.org/w/index.php?title=G._V._Raja&oldid=916789538',\n",
       "  'g_id': -1})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_SINGLE = convert_df_to_json_set_SINGLE(task_data)\n",
    "len(jsonsets_SINGLE), jsonsets_SINGLE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97e0bf2fdee420fb23a617e55505af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(409,\n",
       " {'claim_id': 'Q73936275$1037AE55-B90E-44EB-8E23-76C390E0C787',\n",
       "  'reference_id': '16e1d8bbc347e2506ac55ef4105c7922a81015ba',\n",
       "  'affirmation': 'MOG Music began work in 2013.',\n",
       "  'reference_url': 'https://en.wikipedia.org/w/index.php?title=MOG_Music&oldid=930246276',\n",
       "  'g_id': -1,\n",
       "  'evidence_list': [{'evidence_id': 1,\n",
       "    'sentence_id': '15;16',\n",
       "    'score': -0.3574801385,\n",
       "    'pre_evidence': 'Singer, songwriter Years active 2013–present Associated acts. Joe Mettle. Ohemaa Mercy. Denzel Prempeh. Nii Okai. Musical artist.',\n",
       "    'evidence': 'Nana Yaw Boakye better known by his stage name MOG Music is a Ghanaian contemporary gospel singer, songwriter and a pastor. Contents. 1 Music career.',\n",
       "    'post_evidence': '2 Discography. 2.1 Live albums. 3 Awards and nomination. 4 References. Music career. MOG Music released his first album in 2016 \"New Wine\" which earned him African Gospel Music awards Nominations for \"Album of the Year\" and \"Discovery of the Year\".'},\n",
       "   {'evidence_id': 4,\n",
       "    'sentence_id': '27;28',\n",
       "    'score': -0.8585193157000001,\n",
       "    'pre_evidence': 'He has collaborated and performed with numerous gospel musicians, including Ohemaa Mercy, Joe Mettle, Denzel Prempeh, Jekalyn Carr, Danny Nettey, Nii Okai, Ron Kenoly. Discography. Live albums.',\n",
       "    'evidence': 'New Wine (2016) Better Me (2018) Selected Singles.',\n",
       "    'post_evidence': 'Be Lifted. Elohim. Fakye. Making It big. Awards and nomination. Year. Event. Award. Nominated Work. Result.'},\n",
       "   {'evidence_id': 3,\n",
       "    'sentence_id': '8;9',\n",
       "    'score': -0.8549956679,\n",
       "    'pre_evidence': 'Latest revision ( diff ) | Newer revision → ( diff ) Jump to navigation. Jump to search. Ghanaian singer.',\n",
       "    'evidence': 'MOG Music Birth name Nana Yaw Boakye Genres Gospel Occupation(s) Singer, songwriter Years active 2013–present Associated acts.',\n",
       "    'post_evidence': 'Joe Mettle. Ohemaa Mercy. Denzel Prempeh. Nii Okai. Musical artist. Nana Yaw Boakye better known by his stage name MOG Music is a Ghanaian contemporary gospel singer, songwriter and a pastor.'},\n",
       "   {'evidence_id': 2,\n",
       "    'sentence_id': '0',\n",
       "    'score': -0.4286392331,\n",
       "    'pre_evidence': '',\n",
       "    'evidence': 'MOG Music.',\n",
       "    'post_evidence': 'From Wikipedia, the free encyclopedia. This is an old revision of this page, as edited by Bonnie13J (talk | contribs) at 05:24, 11 December 2019 (Updated infobox).'},\n",
       "   {'evidence_id': 0,\n",
       "    'sentence_id': '22;23',\n",
       "    'score': -0.1240344569,\n",
       "    'pre_evidence': 'Contents. 1 Music career. 2 Discography. 2.1 Live albums. 3 Awards and nomination. 4 References. Music career.',\n",
       "    'evidence': 'MOG Music released his first album in 2016 \"New Wine\" which earned him African Gospel Music awards Nominations for \"Album of the Year\" and \"Discovery of the Year\". He released a single \"Making it Big\" featuring Sarkodie.',\n",
       "    'post_evidence': 'He has collaborated and performed with numerous gospel musicians, including Ohemaa Mercy, Joe Mettle, Denzel Prempeh, Jekalyn Carr, Danny Nettey, Nii Okai, Ron Kenoly.'}]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonsets_MULTIPLE = convert_df_to_json_set_MULTIPLE(task_data)\n",
    "len(jsonsets_MULTIPLE), jsonsets_MULTIPLE[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Artificial Golden Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not needed for this (for now), but let's keep the code here junst in case\n",
    "```\n",
    "data_df_remaining = data_df[data_df.campaign_group >= 3].reset_index(drop=True)\n",
    "#Taken from not campaigns groups 0,1 and 2\n",
    "```\n",
    "\n",
    "### Generate 45 golden data points with poor fluency\n",
    "\n",
    "Select these apart and then manually alter them\n",
    "\n",
    "```\n",
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=42).reset_index(drop=True))\n",
    "\n",
    "for jsonset in jsonsets:\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2],\n",
    "      \"adequacy\": [0,1,2]\n",
    "    }  \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_FLUENCY'\n",
    "    jsonset['verbalised_claim'] = jsonset['verbalised_claim'] + ' $ALTER THIS'\n",
    "    \n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_fluency.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)\n",
    "```\n",
    "\n",
    "### Generate 45 golden data points with poor adequacy\n",
    "\n",
    "Triples are paired with random verbalisations from other triples\n",
    "\n",
    "```\n",
    "jsonsets = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=24783).reset_index(drop=True))\n",
    "jsonsets_2 = convert_microtask_dataframe_to_json_set(data_df_remaining.sample(45, random_state=1847).reset_index(drop=True))\n",
    "\n",
    "for i, jsonset in enumerate(jsonsets):\n",
    "    jsonset['g_id'] = {\n",
    "      \"fluency\": [0,1,2,3,4,5],\n",
    "      \"adequacy\": [1,2]\n",
    "    }  \n",
    "    \n",
    "    jsonset['claim_id'] = jsonset['claim_id'] '_GD_PLUS_ADEQUACY'    \n",
    "    jsonset['verbalised_claim'] = jsonsets_2[i]['verbalised_claim']\n",
    "\n",
    "with open('data/campaign/batch_1/verbalisations_gd_plus_adequacy.json','w+',encoding='utf8') as f:\n",
    "    json.dump(jsonsets[:golden_standard_quota], f, indent=2,ensure_ascii=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are first converting campaign batch 1! That is campaign groups 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_standard_quota_SINGLE = 45 #should be 45 for the campaign\n",
    "golden_standard_quota_MULTIPLE = 45\n",
    "\n",
    "jsonsets_SINGLE_shuffle = random.Random(42).sample(jsonsets_SINGLE, len(jsonsets_SINGLE))\n",
    "#with open('../data/campaign/single_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_SINGLE_shuffle[:golden_standard_quota_SINGLE],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "#with open('../data/campaign/single.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_SINGLE_shuffle[golden_standard_quota_SINGLE:],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )   \n",
    "    \n",
    "jsonsets_MULTIPLE_shuffle = random.Random(42).sample(jsonsets_MULTIPLE, len(jsonsets_MULTIPLE))\n",
    "#with open('../data/campaign/multiple_gd.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_MULTIPLE_shuffle[:golden_standard_quota_MULTIPLE],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )\n",
    "#with open('../data/campaign/multiple.json','w+',encoding='utf8') as f:\n",
    "#    json.dump(\n",
    "#        jsonsets_MULTIPLE_shuffle[golden_standard_quota_MULTIPLE:],\n",
    "#        f, indent=2,ensure_ascii=False\n",
    "#    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must now annotate the golden standard by doing the following:\n",
    "For each gd reference in the gd json set, go to the \"gd\" field, which should be like this:\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": -1\n",
    "}\n",
    "```\n",
    "\n",
    "and change it to something of the format:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"reference_id\": [random uuid],\n",
    "    \"url\": [an url],\n",
    "    ...,\n",
    "    \"g_id\": [list of allowed values]\n",
    "}\n",
    "```\n",
    "where by allowed values we mean any of the following which could apply to this case:\n",
    "- 0: Supports\n",
    "- 1: Refutes\n",
    "- 2: Neither\n",
    "- 3: Not Sure\n",
    "\n",
    "If any golden data case is ambiguous, exchange it for a non-gd case and **save to a x_gd_2.json** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Task Sets\n",
    "\n",
    "**Make sure you have annotated the GD before proceeding here**\n",
    "\n",
    "**Make sure you have also filtered the non-GD for API-verifiable examples**\n",
    "\n",
    "Now we take 4 non_gd references and 2 gd references and pack them into task sets of 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [SINGLE]\n",
      "Gd len: 45\n",
      "Non-Gd len: 2000\n",
      "Generated 500 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [22, 22, 22, 22, 23, 22, 22, 22, 23, 22, 23, 22, 23, 23, 23, 22, 23, 22, 22, 22, 22, 22, 22, 22, 23, 22, 22, 23, 22, 23, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\n",
      "\n",
      "Estimated costs: $1500.0\n"
     ]
    }
   ],
   "source": [
    "cost_single_task = 0.5\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 22 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "print('Generating Task Sets [SINGLE]')\n",
    "with open('../data/campaign/single_gd.json','r',encoding='utf8') as f:\n",
    "    single_gd = json.load(f)\n",
    "    print('Gd len:',len(single_gd))\n",
    "with open('../data/campaign/single.json','r',encoding='utf8') as f:\n",
    "    single = json.load(f)\n",
    "    print('Non-Gd len:',len(single))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(single, single_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_single_task*1.2}')\n",
    "\n",
    "with open('../data/campaign/single_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Task Sets [MULTIPLE]\n",
      "Gd len: 45\n",
      "Non-Gd len: 364\n",
      "Generated 91 tasks, each verbalisation appearance counter is: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Each golden data appearance counter is: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "Estimated costs: $546.0\n"
     ]
    }
   ],
   "source": [
    "cost_multiple_task = 1\n",
    "maxSingleInstances = 1 #MAX TIMES ANY SUBTASK APPEARS AMONG THE TASK SETS\n",
    "maxSingleInstances_gd = 4 #MAX TIMES ANY GOLDEN SUBTASK APPEARS AMONG THE TASK SETS\n",
    "\n",
    "\n",
    "print('Generating Task Sets [MULTIPLE]')\n",
    "with open('../data/campaign/multiple_gd.json','r',encoding='utf8') as f:\n",
    "    verbalisations_gd = json.load(f)\n",
    "    print('Gd len:',len(verbalisations_gd))\n",
    "with open('../data/campaign/multiple.json','r',encoding='utf8') as f:\n",
    "    verbalisations = json.load(f)\n",
    "    print('Non-Gd len:',len(verbalisations))\n",
    "\n",
    "ds, c, c_gd = generateIDdTaskSets(verbalisations, verbalisations_gd, maxSingleInstances, maxSingleInstances_gd)\n",
    "print('Generated {} tasks, each verbalisation appearance counter is: {}'.format(len(ds), c))\n",
    "print('Each golden data appearance counter is: {}'.format(c_gd))\n",
    "print()\n",
    "print(f'Estimated costs: ${len(ds)*5*cost_multiple_task*1.2}')\n",
    "\n",
    "with open('../data/campaign/multiple_TaskSets.json','w+',encoding='utf8') as f:\n",
    "    json.dump(ds,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total cost is $2046'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Total cost is ${1500+546}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
